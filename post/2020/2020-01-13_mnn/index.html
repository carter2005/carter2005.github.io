<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>mnn简介 - carter&#39;s blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="carter2005" /><meta name="description" content="简介 MNN是一个轻量级的深度神经网络推理引擎，在端侧加载深度神经网络模型进行推理预测。目前，MNN已经在阿里巴巴的手机淘宝、手机天猫、优酷等" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.77.0 with theme even" />


<link rel="canonical" href="https://carter2005.github.io/post/2020/2020-01-13_mnn/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<link href="/sass/main.min.8c3cbcb0324c2bb4875ceccba4007cbad4b4ac8377f33af9953c3e7684534a50.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="mnn简介" />
<meta property="og:description" content="简介 MNN是一个轻量级的深度神经网络推理引擎，在端侧加载深度神经网络模型进行推理预测。目前，MNN已经在阿里巴巴的手机淘宝、手机天猫、优酷等" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://carter2005.github.io/post/2020/2020-01-13_mnn/" />
<meta property="article:published_time" content="2020-01-13T10:21:20+08:00" />
<meta property="article:modified_time" content="2020-01-13T10:21:20+08:00" />
<meta itemprop="name" content="mnn简介">
<meta itemprop="description" content="简介 MNN是一个轻量级的深度神经网络推理引擎，在端侧加载深度神经网络模型进行推理预测。目前，MNN已经在阿里巴巴的手机淘宝、手机天猫、优酷等">
<meta itemprop="datePublished" content="2020-01-13T10:21:20+08:00" />
<meta itemprop="dateModified" content="2020-01-13T10:21:20+08:00" />
<meta itemprop="wordCount" content="14212">



<meta itemprop="keywords" content="training," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="mnn简介"/>
<meta name="twitter:description" content="简介 MNN是一个轻量级的深度神经网络推理引擎，在端侧加载深度神经网络模型进行推理预测。目前，MNN已经在阿里巴巴的手机淘宝、手机天猫、优酷等"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">carter&#39;s blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">carter&#39;s blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">mnn简介</h1>

      <div class="post-meta">
        <span class="post-time"> 2020-01-13 </span>
        <div class="post-category">
            <a href="/categories/ai/"> AI </a>
            </div>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次阅读 </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#轻量性">轻量性</a></li>
        <li><a href="#通用性">通用性</a></li>
        <li><a href="#高性能">高性能</a></li>
        <li><a href="#易用性">易用性</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#1-添加模型描述">1. 添加模型描述</a>
      <ul>
        <li><a href="#添加算子类型">添加算子类型</a></li>
        <li><a href="#添加算子参数描述">添加算子参数描述</a></li>
      </ul>
    </li>
    <li><a href="#2-添加模型转换">2. 添加模型转换</a>
      <ul>
        <li><a href="#tensorflow模型转换">TensorFlow模型转换</a></li>
        <li><a href="#tensorflow-lite模型转换">TensorFlow Lite模型转换</a></li>
        <li><a href="#caffe模型转换">Caffe模型转换</a></li>
        <li><a href="#onnx模型转换">ONNX模型转换</a></li>
      </ul>
    </li>
    <li><a href="#3-添加维度计算">3. 添加维度计算</a></li>
    <li><a href="#4-添加实现">4. 添加实现</a>
      <ul>
        <li><a href="#添加cpu实现">添加CPU实现</a></li>
        <li><a href="#添加metal实现">添加Metal实现</a></li>
        <li><a href="#添加vulkan实现">添加Vulkan实现</a></li>
        <li><a href="#添加opencl实现">添加OpenCL实现</a></li>
        <li><a href="#添加opengl实现">添加OpenGL实现</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#原理">原理</a></li>
    <li><a href="#mnn实现">MNN实现</a></li>
  </ul>

  <ul>
    <li><a href="#backend">backend</a>
      <ul>
        <li><a href="#arm82">arm82</a></li>
        <li><a href="#cpu">cpu</a></li>
        <li><a href="#open-vulkan-metal">open**, vulkan, metal</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#功能介绍">功能介绍</a></li>
        <li><a href="#mnn实现-1">mnn实现</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#概念">概念</a></li>
    <li><a href="#机制">机制</a></li>
    <li><a href="#converter实现">converter实现</a></li>
    <li><a href="#interpreter实现">interpreter实现</a>
      <ul>
        <li><a href="#cpu-backend">CPU backend</a></li>
        <li><a href="#metal-backend">metal backend</a></li>
        <li><a href="#vulkan-backend">vulkan backend</a></li>
        <li><a href="#opencl-backend">opencl backend</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h1 id="简介">简介</h1>
<p><img src="/post/image/mnn.png" alt="banner.png"></p>
<p><a href="https://github.com/alibaba/MNN">MNN</a>是一个轻量级的深度神经网络推理引擎，在端侧加载深度神经网络模型进行推理预测。目前，MNN已经在阿里巴巴的手机淘宝、手机天猫、优酷等20多个App中使用，覆盖直播、短视频、搜索推荐、商品图像搜索、互动营销、权益发放、安全风控等场景。此外，IoT等场景下也有若干应用。</p>
<h1 id="特点">特点</h1>
<p><img src="/post/image/mnn_status.png" alt="img"></p>
<p><img src="/post/image/mnn_design.png" alt="img"></p>
<p><img src="/post/image/mnn_convert.png" alt="img"></p>
<p><img src="/post/image/mnn_quantization.png" alt="img"></p>
<p><img src="/post/image/mnn_character.png" alt="img"></p>
<h3 id="轻量性">轻量性</h3>
<ul>
<li>针对端侧设备特点深度定制和裁剪，无任何依赖，可以方便地部署到移动设备和各种嵌入式设备中。</li>
<li>iOS平台：armv7+arm64静态库大小5MB左右，链接生成可执行文件增加大小620KB左右，metallib文件600KB左右。</li>
<li>Android平台：so大小400KB左右，OpenCL库400KB左右，Vulkan库400KB左右。</li>
</ul>
<h3 id="通用性">通用性</h3>
<ul>
<li>支持<code>Tensorflow</code>、<code>Caffe</code>、<code>ONNX</code>等主流模型文件格式，支持<code>CNN</code>、<code>RNN</code>、<code>GAN</code>等常用网络。</li>
<li>支持86个<code>Tensorflow</code>Op、34个<code>Caffe</code>Op；各计算设备支持的MNN Op数：CPU 71个，Metal 55个，OpenCL 29个，Vulkan 31个。</li>
<li>支持iOS 8.0+、Android 4.3+和具有POSIX接口的嵌入式设备。</li>
<li>支持异构设备混合计算，目前支持CPU和GPU，可以动态导入GPU Op插件，替代CPU Op的实现。</li>
</ul>
<h3 id="高性能">高性能</h3>
<ul>
<li>不依赖任何第三方计算库，依靠大量手写汇编实现核心运算，充分发挥ARM CPU的算力。</li>
<li>iOS设备上可以开启GPU加速（Metal），常用模型上快于苹果原生的CoreML。</li>
<li>Android上提供了<code>OpenCL</code>、<code>Vulkan</code>、<code>OpenGL</code>三套方案，尽可能多地满足设备需求，针对主流GPU（<code>Adreno</code>和<code>Mali</code>）做了深度调优。</li>
<li>卷积、转置卷积算法高效稳定，对于任意形状的卷积均能高效运行，广泛运用了 Winograd 卷积算法，对3x3 -&gt; 7x7之类的对称卷积有高效的实现。</li>
<li>针对ARM v8.2的新架构额外作了优化，新设备可利用半精度计算的特性进一步提速。</li>
</ul>
<h3 id="易用性">易用性</h3>
<ul>
<li>有高效的图像处理模块，覆盖常见的形变、转换等需求，一般情况下，无需额外引入libyuv或opencv库处理图像。</li>
<li>支持回调机制，可以在网络运行中插入回调，提取数据或者控制运行走向。</li>
<li>支持只运行网络中的一部分，或者指定CPU和GPU间并行运行。</li>
</ul>
<h1 id="架构">架构</h1>
<p><img src="/post/image/mnn_architecture.png" alt="architecture.png"></p>
<p>MNN可以分为Converter和Interpreter两部分。</p>
<p>Converter由Frontends和Graph Optimize构成。前者负责支持不同的训练框架，MNN当前支持Tensorflow(Lite)、Caffe和ONNX(PyTorch/MXNet的模型可先转为ONNX模型再转到MNN)；后者通过算子融合、算子替代、布局调整等方式优化图。</p>
<p>Interpreter由Engine和Backends构成。前者负责模型的加载、计算图的调度；后者包含各计算设备下的内存分配、Op实现。在Engine和Backends中，MNN应用了多种优化方案，包括在卷积和反卷积中应用Winograd算法、在矩阵乘法中应用Strassen算法、低精度计算、Neon优化、手写汇编、多线程优化、内存复用、异构计算等。</p>
<h1 id="用法">用法</h1>
<p><img src="/post/image/mnn_usage.png" alt="concept.png"></p>
<ul>
<li>训练</li>
</ul>
<p>在训练框架上，根据训练数据训练出模型的阶段。虽然当前MNN也提供了训练模型的能力，但主要用于端侧训练或模型调优。在数据量较大时，依然建议使用成熟的训练框架，如TensorFlow、PyTorch等。除了自行训练外，也可以直接利用开源的预训练模型。</p>
<ul>
<li>转换</li>
</ul>
<p>将其他训练框架模型转换为MNN模型的阶段。MNN当前支持Tensorflow(Lite)、Caffe和ONNX的模型转换。模型转换工具可以参考<a href="https://www.yuque.com/mnn/cn/cvrt_linux">编译文档</a>和<a href="https://www.yuque.com/mnn/cn/model_convert">使用说明</a>。支持转换的算子，可以参考<a href="https://www.yuque.com/mnn/en/ops">算子列表文档</a>；在遇到不支持的算子时，可以尝试<a href="https://www.yuque.com/mnn/cn/customize_op">自定义算子</a>，或在Github上给我们<a href="https://github.com/alibaba/MNN/issues/74">提交issue</a>。</p>
<p>此外，<a href="https://www.yuque.com/mnn/cn/model_dump">模型打印工具</a>可以用于输出模型结构，辅助调试。</p>
<p>除模型转换外，MNN也提供了<a href="https://www.yuque.com/mnn/cn/tool_quantize">模型量化工具</a>，可以对浮点模型进行量化压缩。</p>
<ul>
<li>推理</li>
</ul>
<p>在端侧加载MNN模型进行推理的阶段。端侧运行库的编译请参考各平台的编译文档：<a href="https://www.yuque.com/mnn/cn/build_ios">iOS</a>、<a href="https://www.yuque.com/mnn/cn/build_android">Android</a>、<a href="https://www.yuque.com/mnn/cn/build_linux">Linux/macOS/Ubuntu</a>、<a href="https://www.yuque.com/mnn/cn/build_windows">Windows</a>。我们提供了<a href="https://github.com/alibaba/MNN/tree/master/doc/API">API接口文档</a>，也详细说明了<a href="https://www.yuque.com/mnn/cn/create_session">会话创建</a>、<a href="https://www.yuque.com/mnn/cn/input">数据输入</a>、<a href="https://www.yuque.com/mnn/cn/run_session">执行推理</a>、<a href="https://www.yuque.com/mnn/cn/output">数据输出</a>相关的接口和参数。</p>
<h1 id="自定义算子">自定义算子</h1>
<h2 id="1-添加模型描述">1. 添加模型描述</h2>
<blockquote>
<p>若添加的算子不在MNN的算子列表中，需要添加模型描述</p>
<p>修改完模型描述后，需要调用generate脚本重新生成模型描述头文件。</p>
</blockquote>
<h3 id="添加算子类型">添加算子类型</h3>
<p>在<code>schema/default/MNN.fbs</code>文件的OpType列表里追加算子名称，如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">enum OpType : int {
    AbsVal,
    QuantizedAdd,
    ...
    MyCustomOp
}
</code></pre></td></tr></table>
</div>
</div><h3 id="添加算子参数描述">添加算子参数描述</h3>
<p>如果算子不包含参数，则可以略过这一步。</p>
<p>首先，在<code>schema/default/MNN.fbs</code>文件的OpParameter列表里追加算子参数名称，如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">union OpParameter {
    QuantizedAdd,
    ArgMax,
    AsString,
    ...
    MyCustomOpParam
}
</code></pre></td></tr></table>
</div>
</div><p>而后，添加参数描述。如果算子来自Caffe，选择<code>CaffeOps.fbs</code>；如果算子来自TensorFlow，就使用<code>TensorflowOp.fbs</code>。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">table MyCustomOpParam {
    padX:int;
    padY:int;
    kernelX:int;
    kernelY:int;
    strideX:int;
    strideY:int;
    dataType:DataType=DT_FLOAT;
}
</code></pre></td></tr></table>
</div>
</div><h2 id="2-添加模型转换">2. 添加模型转换</h2>
<blockquote>
<p>用户可根据自己使用的框架，选择对应的模型转换模块去添加算子转换的支持</p>
<p>添加完模型转换后，需要重新cmake。</p>
</blockquote>
<p>目前，MNN支持TensorFlow、TensorFlow Lite、Caffe和ONNX模型格式的转换。</p>
<h3 id="tensorflow模型转换">TensorFlow模型转换</h3>
<p>\1. 添加转换类</p>
<p>在<code>tools/converter/source/tensorflow</code>下添加<code>MyCustomOpTf.cpp</code>。可以直接声明转换类，也可以利用宏定义简化代码。</p>
<p>直接声明示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class MyCustomOpTf : public tfOpConverter {                                             
    public:                                                                       
        virtual void run(MNN::OpT *dstOp, TmpNode *srcNode, TmpGraph *tempGraph);
        MyCustomOpTf() {}                                                                   
        virtual ~MyCustomOpTf() {}                                                          
        virtual MNN::OpType opType();                                             
        virtual MNN::OpParameter type();                                          
}
</code></pre></td></tr></table>
</div>
</div><p>等效宏定义示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">DECLARE_OP_CONVERTER(MyCustomOpTf);
</code></pre></td></tr></table>
</div>
</div><p>需要实现<code>run</code>、析构、<code>opType</code>和<code>type</code>函数。其中，<code>run</code>函数用于解析模型的proto文件得到参数，然后赋值给flatbuffer自定义参数。参数<code>srcNode</code>保存有输入输出节点信息，可以根据输入输出节点在<code>tempGraph</code>中找到<code>TmpNode</code>。调用函数<code>find_attr_value(const tensorflow::NodeDef&amp; node, const char* key, tensorflow::AttrValue&amp; value)</code>获得对应参数的值。</p>
<p>注册转换类：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">REGISTER_CONVERTER(MyCustomOpTf, MyCustomOp);
</code></pre></td></tr></table>
</div>
</div><p>\2. 添加映射</p>
<p>在<code>OpMapper.hpp</code>中添加相应的TensorFlow Op名字到MNN Op名字的映射：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">{&#34;OpName1&#34;, MNN::OpType_MyCustomOp},
{&#34;OpName2&#34;, MNN::OpType_MyCustomOp},
</code></pre></td></tr></table>
</div>
</div><p>\3. 处理Op附带的Const</p>
<p>如果Const不作为此Op的参数，而是看成一个单独的Op，可以忽略此步骤；如果Op要把Const当成参数，要在文件<code>TmpGraph.cpp</code>里修改函数<code>_genMinGraph()</code>，把相应Const节点的<code>isCovered</code>属性设置为true。</p>
<h3 id="tensorflow-lite模型转换">TensorFlow Lite模型转换</h3>
<p>\1. 添加转换类</p>
<p>在<code>tools/converter/source/tflite</code>下添加<code>MyCustomOpTflite.cpp</code>。</p>
<p>宏定义示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">DECLARE_OP_COVERTER(MyCustomOpTflite);
</code></pre></td></tr></table>
</div>
</div><p>需要实现函数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">MyCustomOpTflite::opType(bool quantizedModel);
MyCustomOpTflite::type(bool quantizedModel);
MyCustomOpTflite::run(MNN::OpT *dstOp, 
                      const std::unique_ptr&lt;tflite::OperatorT&gt; &amp;tfliteOp, 
                      const std::vector&lt;std::unique_ptr&lt;tflite::TensorT&gt; &gt; &amp;tfliteTensors,
                      const std::vector&lt;std::unique_ptr&lt;tflite::BufferT&gt; &gt; &amp;tfliteModelBuffer,
                      const std::vector&lt;std::unique_ptr&lt;tflite::OperatorCodeT&gt; &gt; &amp;tfliteOpSet,
                      bool quantizedModel)
</code></pre></td></tr></table>
</div>
</div><p>其中，<code>run</code>函数相比TensorFlow的版本，多一个<code>quantizedModel</code>参数。若<code>qu</code><img src="" alt="img"><code>antizedModel</code>为true，则模型为量化模型，需转为相应的量化Op；若为false，转为浮点Op。在run函数中需要设置输入、输出tensor的index：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">// set input output index
dstOp-&gt;inputIndexes.resize(1);
dstOp-&gt;outputIndexes.resize(1);
dstOp-&gt;inputIndexes[0]  = tfliteOp-&gt;inputs[0];
dstOp-&gt;outputIndexes[0] = tfliteOp-&gt;outputs[0];
</code></pre></td></tr></table>
</div>
</div><p>注册转换类：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">using namespace tflite;
REGISTER_CONVERTER(MyCustomOpTflite, BuiltinOperator_OPName);
</code></pre></td></tr></table>
</div>
</div><h3 id="caffe模型转换">Caffe模型转换</h3>
<p>\1. 添加转换类</p>
<p>在<code>/tools/converter/source/caffe</code>下添加MyCustomOp.cpp。</p>
<p>类声明示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class MyCustomOp : public OpConverter {
public:
    virtual void run(MNN::OpT* dstOp, 
                     const caffe::LayerParameter&amp; parameters, 
                     const caffe::LayerParameter&amp; weight);
    MyCustomOp() {}
    virtual ~MyCustomOp() {}
    virtual MNN::OpType opType();
    virtual MNN::OpParameter type();
};
</code></pre></td></tr></table>
</div>
</div><p>实现<code>run</code>、<code>opType</code>、<code>type</code>函数，在<code>run</code>函数中解析caffe参数得到具体参数。其中参数parameters保存有Op的参数信息，weight保存有卷积、BN等数据参数。</p>
<p>注册转换类：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">static OpConverterRegister&lt;MyCustomOp&gt; a(&#34;MyCustomOp&#34;);
</code></pre></td></tr></table>
</div>
</div><h3 id="onnx模型转换">ONNX模型转换</h3>
<p>\1. 添加转换类</p>
<p>在<code>/tools/converter/source/onnx</code>下添加MyCustomOpOnnx.cpp。</p>
<p>类声明示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">DECLARE_OP_CONVERTER(MyCustomOpOnnx);
</code></pre></td></tr></table>
</div>
</div><p>需要实现函数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">MNN::OpType MyCustomOpOnnx::opType();
MNN::OpParameter MyCustomOpOnnx::type();
void MyCustomOpOnnx::run(MNN::OpT* dstOp, 
                         const onnx::NodeProto* onnxNode, 
                         std::vector&lt;const onnx::TensorProto*&gt; initializers);
</code></pre></td></tr></table>
</div>
</div><p><code>run</code>函数中，onnxNode即onnx原始节点信息，权重等数据信息需从initializers取。</p>
<p>注册转换类：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">REGISTER_CONVERTER(MyCustomOpOnnx, MyCustomOp);
</code></pre></td></tr></table>
</div>
</div><h2 id="3-添加维度计算">3. 添加维度计算</h2>
<blockquote>
<p>如果该Op的输出Tensor大小与第1个输入Tensor一致，并且不需要分析FLOPS，可以跳过这步。</p>
<p>添加完形状计算代码后，需要重新cmake。</p>
</blockquote>
<p>\1. 添加计算类</p>
<p>在<code>/source/shape</code>下添加ShapeMyCustomOp.cpp：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class MyCustomOpSizeComputer : public SizeComputer {
public:
    virtual bool onComputeSize(const MNN::Op* op, const std::vector&lt;Tensor*&gt;&amp; inputs,
                               const std::vector&lt;Tensor*&gt;&amp; outputs) const override {
        // set tensor-&gt;buffer.type
        //                   .dimensions
        //                   .dim[x].extent
        //                   .dim[x].stride
        //                   .dim[x].flag
        return true;
    }
    virtual float onComputeFlops(const MNN::Op* op, 
                                 const std::vector&lt;Tensor*&gt;&amp; inputs,
                                 const std::vector&lt;Tensor*&gt;&amp; outputs) const {
        return flops_for_calc_output_from_input;
    }
};
</code></pre></td></tr></table>
</div>
</div><p>在<code>onComputeSize</code>函数中，根据输入tensor的维度信息，计算输出tensor的维度信息，并设置输出tensor的数据类型。计算完成后返回true；若输入维度信息未知返回false。</p>
<p>在<code>onComputeFlops</code>函数中，根据输入、输出tensor的维度信息，返回总计算量。</p>
<p>注册计算类：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">REGISTER_SHAPE(MyCustomOpSizeComputer, OpType_MyCustomOp);
</code></pre></td></tr></table>
</div>
</div><h2 id="4-添加实现">4. 添加实现</h2>
<blockquote>
<p>添加完算子实现后，需要重新cmake。</p>
</blockquote>
<h3 id="添加cpu实现">添加CPU实现</h3>
<p>在<code>source/backend/CPU</code>目录下添加<code>CPUMyCustomOp.hpp</code>、<code>CPUMyCustomOp.cpp</code>。</p>
<p>\1. 实现类声明</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class CPUMyCustomOp : public Execution {
public:
    // 若执行onExecute需要使用缓存，在此函数中申请，若无可不声明
    virtual ErrorCode onResize(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                               const std::vector&lt;Tensor *&gt; &amp;outputs) override;
    // 具体的Op执行函数
    virtual ErrorCode onExecute(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                                const std::vector&lt;Tensor *&gt; &amp;outputs) override;
};
</code></pre></td></tr></table>
</div>
</div><p>\2. 实现<code>onResize</code>和<code>onExecute</code></p>
<p>在<code>onResize</code>中，调用<code>backend()-&gt;onAcquireBuffer(&amp;mCache, Backend::DYNAMIC)</code>进行缓存的申请，调用<code>backend()-&gt;onReleaseBuffer(&amp;mCache, Backend::DYNAMIC)</code>回收缓存。释放后的内存可以被复用。</p>
<p>在<code>onExecute</code>中，做必要的输入的检查，有利于提前发现问题。若执行完毕正确返回NO_ERROR。</p>
<p>\3. 注册实现类</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class CPUMyCustomOpCreator : public CPUBackend::Creator {
public:
    virtual Execution *onCreate(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                                const std::vector&lt;Tensor *&gt; &amp;outputs, 
                                const MNN::Op *op,
                                Backend *backend) const override {
        return new CPUMyCustomOp(backend);
    }
};
REGISTER_CPU_OP_CREATOR(CPUMyCustomOpCreator, OpType_MyCustomOp);
</code></pre></td></tr></table>
</div>
</div><h3 id="添加metal实现">添加Metal实现</h3>
<p>\1. 添加Shader</p>
<p>在<code>source/backend/Metal</code>目录下添加<code>MetalMyCustomOp.metal</code>，并添加进Xcode工程。metal可以参考目录下已有实现。</p>
<p>\2. 实现类声明</p>
<p>在<code>source/backend/Metal</code>目录下添加<code>MetalMyCustomOp.hpp</code>和<code>MetalMyCustomOp.cpp</code>，并添加进Xcode工程：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class MetalMyCustomOp : public Execution {
public:
    virtual ErrorCode onResize(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                               const std::vector&lt;Tensor *&gt; &amp;outputs) override;
    virtual ErrorCode onExecute(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                                const std::vector&lt;Tensor *&gt; &amp;outputs) override;
};
</code></pre></td></tr></table>
</div>
</div><p>\3. 实现<code>onResize</code>和<code>onExecute</code></p>
<p>不同于CPU Tensor将数据存储在host指针中，Metal数据指针存放在<code>deviceId</code>中，deviceId上存储的是<code>id&lt;MTLBuffer&gt;</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">auto buffer = (__bridge id&lt;MTLBuffer&gt;)(void *)tensor-&gt;deviceId();
</code></pre></td></tr></table>
</div>
</div><p>Metal Op的特定参数等可以通过<code>id&lt;MTLBuffer&gt;</code>存储。buffer数据类型可以与tensor不同，buffer甚至可以混合多种数据类型，只需保证创建时指定了正确的长度即可。例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">auto buffer = [context newDeviceBuffer:2 * sizeof(int) + 2 * sizeof(__fp16) access:CPUWriteOnly];
((__fp16 *)buffer.contents)[0] = mAlpha / mLocalSize;  // alpha
((__fp16 *)buffer.contents)[1] = mBeta;                // beta
((int *)buffer.contents)[1] = mLocalSize;              // local size
((int *)buffer.contents)[2] = inputs[0]-&gt;channel();    // channel
</code></pre></td></tr></table>
</div>
</div><p>在创建buffer时，需要指定访问控制权限。目前共有三种权限：</p>
<ul>
<li>
<ul>
<li><code>CPUReadWrite</code>，数据在CPU/GPU间共享存储，一般用于device buffer；</li>
<li><code>CPUWriteOnly</code>，数据通过CPU写入后不再读取，一般用于参数buffer；</li>
<li><code>CPUTransparent</code>，数据只在GPU中，一般用于heap buffer；</li>
</ul>
</li>
</ul>
<p><strong>MNNMetalContext</strong>在创建buffer上，有两套相近的接口，区别只在数据的生命周期上：</p>
<ul>
<li>
<ul>
<li>device占用的内存在单次推理过程中都不会被复用；</li>
<li>而heap占用的内存，在调用<code>-[MNNMetalContext releaseHeapBuffer:]</code>之后，可以被其他Op复用；</li>
</ul>
</li>
</ul>
<p>一般而言，heap只会与<strong>CPUTransparent</strong>一起使用。<em>heap实际只在iOS 10+上有效，iOS 9-上会回退到device上。</em></p>
<p>使用Metal时，<strong>如非特殊情况，禁止自行创建device和library</strong>。加载library、编译function都是耗时行为，<strong>MNNMetalContext</strong>上做了必要的缓存优化。通过context执行Metal的示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">auto context   = (__bridge MNNMetalContext *)backend-&gt;context();
auto kernel    = /* metal kernel name NSString */;
auto encoder   = [context encoder];
auto bandwidth = [context load:kernel encoder:encoder];
/* encoder set buffer(s)/sampler(s) */
[context dispatchEncoder:encoder 
                 threads:{x, y, z}
      maxThreadsPerGroup:maxThreadsPerThreadgroup]; // recommended way to dispatch
[encoder endEncoding];
</code></pre></td></tr></table>
</div>
</div><p>\4. 注册实现类</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class MetalMyCustomOpCreator : public MetalBackend::Creator {
public:
    virtual Execution *onCreate(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                                const MNN::Op *op, Backend *backend) const {
        return new MetalMyCustomOp(backend);
    }
};
REGISTER_METAL_OP_CREATOR(MetalMyCustomOpCreator, OpType_MyCustomOp);
</code></pre></td></tr></table>
</div>
</div><h3 id="添加vulkan实现">添加Vulkan实现</h3>
<p>\1. 添加Shader</p>
<p>在<code>source/backend/vulkan/execution/glsl</code><img src="" alt="img">目录下添加具体的shader(*.comp)。若输入内存布局为<code>NC4HW4</code>，则按<code>image</code>实现，否则采用buffer实现。可以参考目录下已有实现。然后，执行<code>makeshader.py</code>脚本编译Shader。</p>
<p>\2. 实现类声明</p>
<p>在目录<code>source/backend/vulkan/execution/</code>下添加<code>VulkanMyCustomOp.hpp</code>和<code>VulkanMyCustomOp.cpp</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class VulkanMyCustomOp : public VulkanBasicExecution {
public:
    VulkanMyCustomOp(const Op* op, Backend* bn);
    virtual ~VulkanMyCustomOp();
    ErrorCode onEncode(const std::vector&lt;Tensor*&gt;&amp; inputs, 
                       const std::vector&lt;Tensor*&gt;&amp; outputs,
                       const VulkanCommandPool::Buffer* cmdBuffer) override;
private:
    // GPU Shader所需的参数
    std::shared_ptr&lt;VulkanBuffer&gt; mConstBuffer;
    // Pipeline
    const VulkanPipeline* mPipeline;
    // Layout Descriptor Set
    std::shared_ptr&lt;VulkanPipeline::DescriptorSet&gt; mDescriptorSet;
};
</code></pre></td></tr></table>
</div>
</div><p>\3. 实现</p>
<p>实现函数<code>onEncode</code>，首先需要做内存布局检查：若为<code>NC4HW4</code>，则Shader用image实现，否则用buffer。执行完毕返回NO_ERROR。</p>
<p>\4. 注册实现类</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class VulkanMyCustomOpCreator : public VulkanBackend::Creator {
public:
    virtual Execution* onCreate(const std::vector&lt;Tensor*&gt;&amp; inputs, 
                                const MNN::Op* op,
                                Backend* backend) const override {
        return new VulkanMyCustomOp(op, backend);
    }
};
static bool gResistor = []() {
    VulkanBackend::addCreator(OpType_MyCustomOp, new VulkanMyCustomOpCreator);
    return true;
}();
</code></pre></td></tr></table>
</div>
</div><h3 id="添加opencl实现">添加OpenCL实现</h3>
<p>\1. 添加Kernel</p>
<p>在<code>source/backend/opencl/execution/cl</code>目录添加具体的kernel(*.cl)。目前feature map均使用<code>image2d</code>实现。可以参考目录下已有实现。然后执行<code>opencl_codegen.py</code>来生成kernel映射。</p>
<p>\2. 实现类声明</p>
<p>在目录<code>source/backend/opencl/execution/</code>下添加<code>MyCustomOp.h</code>和<code>MyCustomOp.cpp</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">template &lt;typename T&gt;
class MyCustomOp : public Execution {
public:
    virtual ErrorCode onResize(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                               const std::vector&lt;Tensor *&gt; &amp;outputs) override;
    virtual ErrorCode onExecute(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                                const std::vector&lt;Tensor *&gt; &amp;outputs) override;
};
</code></pre></td></tr></table>
</div>
</div><p>\3. 实现</p>
<p>实现函数<code>onResize</code>(可选)、<code>onExecute</code>。执行完毕返回NO_ERROR。</p>
<p>\4. 注册实现类</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">OpenCLCreatorRegister&lt;TypedCreator&lt;MyCustomOp&lt;cl_data_t&gt;&gt;&gt; __my_custom_op(OpType_MyCustomOp);
</code></pre></td></tr></table>
</div>
</div><h3 id="添加opengl实现">添加OpenGL实现</h3>
<p>\1. 添加Shader</p>
<p>在<code>source/backend/opengl/glsl</code>下添加具体的shader(*.glsl)，不用加文件头，feature map 均采用<code>image3d</code>表示。可以参考目录下已有实现。而后，在<code>source/backend/opengl</code>目录下执行<code>makeshader.py</code>。</p>
<p>\2. 添加Executor</p>
<p>在<code>source/backend/opengl/execution/</code>目录下添加<code>GLMyCustomOp.h</code>和<code>GLMyCustomOp.cpp</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">class GLMyCustomOp : public Execution {
public:
    GLMyCustomOp(const std::vector&lt;Tensor *&gt; &amp;inputs, const Op *op, Backend *bn);
    virtual ~GLMyCustomOp();
    virtual ErrorCode onExecute(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                                const std::vector&lt;Tensor *&gt; &amp;outputs) override;
    virtual ErrorCode onResize(const std::vector&lt;Tensor *&gt; &amp;inputs, 
                               const std::vector&lt;Tensor *&gt; &amp;outputs) override;

private:
    std::shared_ptr&lt;GLProgram&gt; mProgram;
};
</code></pre></td></tr></table>
</div>
</div><p>\3. 实现</p>
<p>实现函数<code>onResize</code>(可选)、<code>onExecute</code>。执行完毕返回NO_ERROR。</p>
<p>\4. 注册实现类-</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">GLCreatorRegister&lt;TypedCreator&lt;GLMyCustomOp&gt;&gt; __my_custom_op(OpType_MyCustomOp);
</code></pre></td></tr></table>
</div>
</div><h1 id="矩阵相乘">矩阵相乘</h1>
<h2 id="原理">原理</h2>
<p>MNN的实现也是简单的按行数据并行处理。</p>
<p>矩阵乘法
矩阵乘法的目的是完成一个计算：C = A * B，其中A是h * k, B是k * w，所以C是h * w。</p>
<p><img src="/post/image/mnn_matrix_multiply.png" alt="å¨è¿éæå¥å¾çæè¿°"></p>
<p>常用的方式是分行处理，对于C的第一行，可以按如下方式处理：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">C(0,j) += A(0,i)*B(i,j)
</code></pre></td></tr></table>
</div>
</div><p>对于行主序矩阵，每一行的数据是连续存储的，我们自然可以考虑使用SIMD指令，一次处理4个(假设是Float32)数据的相乘:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">float32x4_t a0   = vdupq_n_f32(aLine[i]);
float32x4_t b0   = vld1q_f32(bLine);
float32x4_t sum0 = vdupq_n_f32(0.0);
sum0             = vmlaq_f32(sum0, a0, b0);
vst1q_f32(cLine, sum0);
</code></pre></td></tr></table>
</div>
</div><p>需要注意的一点是，如果w不能被4整除，那么需要处理边界，逐个点进行计算并赋值：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">C(0,j) += A(0,i) * B(i,j)
</code></pre></td></tr></table>
</div>
</div><h2 id="mnn实现">MNN实现</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">void Matrix::multi(Tensor* C, const Tensor* A, const Tensor* B) {
    MNN_ASSERT(NULL != C);
    MNN_ASSERT(NULL != B);
    MNN_ASSERT(NULL != A);

    MNN_ASSERT(2 == C-&gt;dimensions());
    MNN_ASSERT(2 == B-&gt;dimensions());
    MNN_ASSERT(2 == A-&gt;dimensions());

    const auto a = A-&gt;host&lt;float&gt;();
    const auto b = B-&gt;host&lt;float&gt;();
    auto c       = C-&gt;host&lt;float&gt;();

    const int h = A-&gt;length(0);
    const int k = A-&gt;length(1);
    const int w = B-&gt;length(1);

    const int aw = A-&gt;stride(0);
    const int bw = B-&gt;stride(0);
    const int cw = C-&gt;stride(0);

    MNN_ASSERT(k == B-&gt;length(0));

    int y = 0;
    for (; y &lt; h; ++y) {
        int x            = 0;
        const auto aLine = a + y * aw;
        auto cLine       = c + y * cw;
#ifdef MNN_USE_NEON
        // firstly, compute 16 together
        for (; x &lt;= w - 16; x += 16) {
            auto bColumn     = b + x;
            float32x4_t sum0 = vdupq_n_f32(0.0);
            float32x4_t sum1 = vdupq_n_f32(0.0);
            float32x4_t sum2 = vdupq_n_f32(0.0);
            float32x4_t sum3 = vdupq_n_f32(0.0);
            for (int i = 0; i &lt; k; ++i) {
                const auto bLine = bColumn + i * bw;
                float32x4_t a0   = vdupq_n_f32(aLine[i]);
                float32x4_t b0   = vld1q_f32(bLine);
                float32x4_t b1   = vld1q_f32(bLine + 4);
                float32x4_t b2   = vld1q_f32(bLine + 8);
                float32x4_t b3   = vld1q_f32(bLine + 12);
                sum0             = vmlaq_f32(sum0, a0, b0);
                sum1             = vmlaq_f32(sum1, a0, b1);
                sum2             = vmlaq_f32(sum2, a0, b2);
                sum3             = vmlaq_f32(sum3, a0, b3);
            }
            vst1q_f32(cLine + x, sum0);
            vst1q_f32(cLine + x + 4, sum1);
            vst1q_f32(cLine + x + 8, sum2);
            vst1q_f32(cLine + x + 12, sum3);
        }
        // secondly, compute 4 together
        for (; x &lt;= w - 4; x += 4) {
            auto bColumn    = b + x;
            float32x4_t sum = vdupq_n_f32(0.0);
            for (int i = 0; i &lt; k; ++i) {
                const auto bLine = bColumn + i * bw;
                float32x4_t a4   = vdupq_n_f32(aLine[i]);
                float32x4_t b4   = vld1q_f32(bLine);
                sum              = vmlaq_f32(sum, a4, b4);
            }
            vst1q_f32(cLine + x, sum);
        }
#endif
        for (; x &lt; w; ++x) {
            auto bColumn = b + x;
            float sum    = 0.0f;
            for (int i = 0; i &lt; k; ++i) {
                sum += aLine[i] * bColumn[i * bw];
            }
            cLine[x] = sum;
        }
    }
}
</code></pre></td></tr></table>
</div>
</div><p>关键部分是MNN_USE_NEON宏包裹的部分，具体的思路，对输出矩阵C进行循环，因为是行主序(每一行连续存储)，所以按行来进行计算，只不过它这里，先按16循环，可以利用流水线，提升效率，然后对于小于16的部分，先4个一组处理，对于小于4的边界部分，逐点处理。</p>
<h1 id="source分析">source分析</h1>
<table>
<thead>
<tr>
<th>内容</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>3rd_party</td>
<td>第三方工具</td>
</tr>
<tr>
<td>benchmark</td>
<td>性能测试工具</td>
</tr>
<tr>
<td>cmake</td>
<td>编译相关</td>
</tr>
<tr>
<td>CMakeLists.txt</td>
<td>编译相关</td>
</tr>
<tr>
<td>demo</td>
<td>demo</td>
</tr>
<tr>
<td>doc</td>
<td>文档</td>
</tr>
<tr>
<td>express</td>
<td></td>
</tr>
<tr>
<td>include</td>
<td>头文件</td>
</tr>
<tr>
<td>project</td>
<td>android，ios，linux工程</td>
</tr>
<tr>
<td>pymnn</td>
<td>python包</td>
</tr>
<tr>
<td>resource</td>
<td>模型，图片等资源</td>
</tr>
<tr>
<td>schema</td>
<td>描述文件，编译相关</td>
</tr>
<tr>
<td>source</td>
<td>核心算法库</td>
</tr>
<tr>
<td>test</td>
<td>测试相关</td>
</tr>
<tr>
<td>tools</td>
<td>converter，quantization等工具</td>
</tr>
</tbody>
</table>
<p>重点关注source目录，source下面有5个目录，分别为</p>
<table>
<thead>
<tr>
<th>目录</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>backend</td>
<td>CPU，GPU加速后端</td>
</tr>
<tr>
<td>core</td>
<td>核心框架，backend，session，pipeline，execution，schedule等框架</td>
</tr>
<tr>
<td>cv</td>
<td>图像库，各种颜色格式，图像格式转换，</td>
</tr>
<tr>
<td>math</td>
<td>matrix，vertex，wingored基本运算</td>
</tr>
<tr>
<td>shape</td>
<td>算子定义</td>
</tr>
</tbody>
</table>
<h2 id="backend">backend</h2>
<h3 id="arm82">arm82</h3>
<p>这个目录下面是arm处理器的优化cpu算子，包含1X1的卷积，矩阵优化汇编等几个优化实现。</p>
<h3 id="cpu">cpu</h3>
<p>通用的cpu后端实现，包含x86的asm，sse，avx等优化实现</p>
<h3 id="open-vulkan-metal">open**, vulkan, metal</h3>
<p>GPU加速方案，api不一样</p>
<h1 id="interp算子">Interp算子</h1>
<h3 id="功能介绍">功能介绍</h3>
<p>图像采样算法：https://blog.csdn.net/LanerGaming/article/details/49207435</p>
<p>重采样效果图：https://clouard.users.greyc.fr/Pantheon/experiments/rescaling/index-en.html</p>
<p>1、格式 VI = interpn(X1,X2,,…,Xn,V,Y1,Y2,…,Yn) %返回由参量X1,X2,…,Xn,V确定的n元函数V=V(X1,X2,…,Xn)在点（Y1,Y2,…,Yn）处的插值。参量Y1,Y2,…,Yn是同型的矩阵或向量。若Y1,Y2,…,Yn是向量，则可以是不同长度，不同方向（行或列）的向量。它们将通过命令ndgrid生成同型的矩阵，再作计算。若点(Y1,Y2,…,Yn)中有位于点（X1,X2,…,Xn）之外的点，则相应地返回特殊变量NaN。</p>
<p>2、VI = interpn(V,Y1,Y2,…,Yn) %缺省地，X1=1:size(V,1)，X2=1:size(V,2)，…，Xn=1:size(V,n)，再按上面的情形计算。</p>
<p>3、VI = interpn(V,ntimes) %作ntimes次递归计算，在V的每两个元素之间插入它们的n维插值。这样，V的阶数将不断增加。interpn(V)等价于interpn(V,1)。</p>
<p>4、VI = interpn(…,method) %用指定的算法method计算：</p>
<ul>
<li><strong><code>bilinear</code></strong>: <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation">Bilinear interpolation.</a> If &lsquo;antialias&rsquo; is true, becomes a hat/tent filter function with radius 1 when downsampling.</li>
<li><strong><code>lanczos3</code></strong>: <a href="https://en.wikipedia.org/wiki/Lanczos_resampling">Lanczos kernel</a> with radius 3. High-quality practical filter but may have some ringing especially on synthetic images.</li>
<li><strong><code>lanczos5</code></strong>: <a href="https://en.wikipedia.org/wiki/Lanczos_resampling">Lanczos kernel</a> with radius 5. Very-high-quality filter but may have stronger ringing.</li>
<li><strong><code>bicubic</code></strong>: <a href="https://en.wikipedia.org/wiki/Bicubic_interpolation">Cubic interpolant</a> of Keys. Equivalent to Catmull-Rom kernel. Reasonably good quality and faster than Lanczos3Kernel, particularly when upsampling.</li>
<li><strong><code>gaussian</code></strong>: <a href="https://en.wikipedia.org/wiki/Gaussian_filter">Gaussian kernel</a> with radius 3, sigma = 1.5 / 3.0.</li>
<li><strong><code>nearest</code></strong>: <a href="https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation">Nearest neighbor interpolation.</a> &lsquo;antialias&rsquo; has no effect when used with nearest neighbor interpolation.</li>
<li><strong><code>area</code></strong>: Anti-aliased resampling with area interpolation. &lsquo;antialias&rsquo; has no effect when used with area interpolation; it always anti-aliases.</li>
<li><strong><code>mitchellcubic</code></strong>: Mitchell-Netravali Cubic non-interpolating filter. For synthetic images (especially those lacking proper prefiltering), less ringing than Keys cubic kernel but less sharp.</li>
</ul>
<h3 id="mnn实现-1">mnn实现</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"><span class="n">ErrorCode</span> <span class="n">CPUInterp</span><span class="o">::</span><span class="n">onExecute</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="o">&amp;</span><span class="n">input</span>  <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">buffer</span><span class="p">();</span>
    <span class="k">auto</span> <span class="o">&amp;</span><span class="n">output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">buffer</span><span class="p">();</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">mResizeType</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Nearstneighbor
</span><span class="c1"></span>        <span class="n">CPUReiseNearstneighborC4</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mWidthScale</span><span class="p">,</span> <span class="n">mHeightScale</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">mResizeType</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// bilinear
</span><span class="c1"></span>        <span class="n">CPUResizeBilinearC4</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mWidthPosition</span><span class="p">.</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">mWidthFactor</span><span class="p">.</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span>
                            <span class="n">mHeightPosition</span><span class="p">.</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">mHeightFactor</span><span class="p">.</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">mLineBuffer</span><span class="p">.</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span>
                            <span class="p">((</span><span class="n">CPUBackend</span> <span class="o">*</span><span class="p">)</span><span class="n">backend</span><span class="p">())</span><span class="o">-&gt;</span><span class="n">threadNumber</span><span class="p">());</span>
    <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">mResizeType</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// cubic
</span><span class="c1"></span>        <span class="n">CPUResizeCubicC4</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">NOT_SUPPORT</span><span class="p">;</span>
        <span class="c1">// not supported
</span><span class="c1"></span>    <span class="p">}</span>
    <span class="k">return</span> <span class="n">NO_ERROR</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">CPUResizeCommon</span><span class="o">::</span><span class="n">CPUReiseNearstneighborC4</span><span class="p">(</span><span class="n">halide_buffer_t</span><span class="o">&amp;</span> <span class="n">input</span><span class="p">,</span> <span class="n">halide_buffer_t</span><span class="o">&amp;</span> <span class="n">output</span><span class="p">,</span> <span class="kt">float</span> <span class="n">wScale</span><span class="p">,</span>
                                               <span class="kt">float</span> <span class="n">hScale</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">batches</span>         <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">inputBatchSize</span>  <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">stride</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">outputBatchSize</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">stride</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">inW</span>             <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">inH</span>             <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">outW</span>            <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">outH</span>            <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">xScaling</span>      <span class="o">=</span> <span class="n">wScale</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">yScaling</span>      <span class="o">=</span> <span class="n">hScale</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">depthQuad</span>       <span class="o">=</span> <span class="n">UP_DIV</span><span class="p">(</span><span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">extent</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>

    <span class="n">AutoStorage</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">linePosition</span><span class="p">(</span><span class="n">outW</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">_linePosition</span> <span class="o">=</span> <span class="n">linePosition</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">outW</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">src_x</span>      <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">xScaling</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">x1</span>           <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">floor</span><span class="p">(</span><span class="n">src_x</span><span class="p">));</span>
        <span class="n">_linePosition</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inW</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">batches</span><span class="p">;</span> <span class="o">++</span><span class="n">b</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">MNN_CONCURRENCY_BEGIN</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">depthQuad</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">auto</span> <span class="n">srcData</span> <span class="o">=</span>
                <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">.</span><span class="n">host</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">inputBatchSize</span> <span class="o">+</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">inW</span> <span class="o">*</span> <span class="n">inH</span><span class="p">;</span>
            <span class="k">auto</span> <span class="n">dstData</span> <span class="o">=</span>
                <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">host</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">outputBatchSize</span> <span class="o">+</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="n">outH</span><span class="p">;</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">dy</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">dy</span> <span class="o">&lt;</span> <span class="n">outH</span><span class="p">;</span> <span class="o">++</span><span class="n">dy</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">float</span> <span class="n">srcY</span>       <span class="o">=</span> <span class="n">dy</span> <span class="o">*</span> <span class="n">yScaling</span><span class="p">;</span>
                <span class="k">const</span> <span class="kt">int</span> <span class="n">y_</span>     <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">floor</span><span class="p">(</span><span class="n">srcY</span><span class="p">)),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
                <span class="k">auto</span> <span class="n">srcDataLine</span> <span class="o">=</span> <span class="n">srcData</span> <span class="o">+</span> <span class="n">inW</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">y_</span><span class="p">;</span>
                <span class="k">auto</span> <span class="n">dstDataLine</span> <span class="o">=</span> <span class="n">dstData</span> <span class="o">+</span> <span class="n">outW</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">dy</span><span class="p">;</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">dx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">dx</span> <span class="o">&lt;</span> <span class="n">outW</span><span class="p">;</span> <span class="o">++</span><span class="n">dx</span><span class="p">)</span> <span class="p">{</span>
                    <span class="o">::</span><span class="n">memcpy</span><span class="p">(</span><span class="n">dstDataLine</span> <span class="o">+</span> <span class="n">dx</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">srcDataLine</span> <span class="o">+</span> <span class="n">_linePosition</span><span class="p">[</span><span class="n">dx</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span><span class="p">);</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="n">MNN_CONCURRENCY_END</span><span class="p">();</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">CPUResizeCommon</span><span class="o">::</span><span class="n">CPUResizeBilinearC4</span><span class="p">(</span><span class="n">halide_buffer_t</span><span class="o">&amp;</span> <span class="n">input</span><span class="p">,</span> <span class="n">halide_buffer_t</span><span class="o">&amp;</span> <span class="n">output</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">widthPosition</span><span class="p">,</span>
                                          <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">widthFactor</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">heightPosition</span><span class="p">,</span>
                                          <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">heightFactor</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">lineBuffer</span><span class="p">,</span> <span class="kt">int</span> <span class="n">threadNumber</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">batches</span>         <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">inputBatchSize</span>  <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">stride</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">outputBatchSize</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">stride</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">inW</span>             <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">inH</span>             <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">outW</span>            <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">outH</span>            <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>

    <span class="kt">int</span> <span class="n">depthQuad</span> <span class="o">=</span> <span class="n">UP_DIV</span><span class="p">(</span><span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">extent</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">batches</span><span class="p">;</span> <span class="o">++</span><span class="n">b</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">auto</span> <span class="n">threadFunction</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">size_t</span> <span class="n">tId</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">tId</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">depthQuad</span><span class="p">;</span> <span class="n">n</span> <span class="o">+=</span> <span class="n">threadNumber</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">auto</span> <span class="n">_lineBuffer</span> <span class="o">=</span> <span class="n">lineBuffer</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="n">tId</span><span class="p">;</span>
                <span class="k">auto</span> <span class="n">_line0</span>      <span class="o">=</span> <span class="n">_lineBuffer</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="mi">0</span><span class="p">;</span>
                <span class="k">auto</span> <span class="n">_line1</span>      <span class="o">=</span> <span class="n">_lineBuffer</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="mi">1</span><span class="p">;</span>
                <span class="kt">int</span> <span class="n">yUsed</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>     <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">};</span>
                <span class="kt">int</span> <span class="n">yCache</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>    <span class="o">=</span> <span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">};</span>

                <span class="kt">float</span><span class="o">*</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>          <span class="o">=</span> <span class="p">{</span><span class="n">_line0</span><span class="p">,</span> <span class="n">_line1</span><span class="p">};</span>
                <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">yCacheStorage</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">_line0</span><span class="p">,</span> <span class="n">_line1</span><span class="p">};</span>

                <span class="k">auto</span> <span class="n">bottomData</span> <span class="o">=</span>
                    <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">.</span><span class="n">host</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">inputBatchSize</span> <span class="o">+</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">n</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">inW</span> <span class="o">*</span> <span class="n">inH</span><span class="p">;</span>
                <span class="k">auto</span> <span class="n">topData</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">host</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">outputBatchSize</span> <span class="o">+</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">n</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="n">outH</span><span class="p">;</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">dy</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">dy</span> <span class="o">&lt;</span> <span class="n">outH</span><span class="p">;</span> <span class="n">dy</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                    <span class="kt">int</span> <span class="n">yp</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
                    <span class="n">yp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">heightPosition</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">dy</span> <span class="o">+</span> <span class="mi">0</span><span class="p">];</span>
                    <span class="n">yp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">heightPosition</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">dy</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
                    <span class="c1">// Search cache
</span><span class="c1"></span>                    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">yUsed</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                    <span class="p">}</span>
                    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
                        <span class="kt">int</span> <span class="n">find</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
                            <span class="k">if</span> <span class="p">(</span><span class="n">yp</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">yCache</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="p">{</span>
                                <span class="n">yUsed</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>      <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
                                <span class="n">yCacheLine</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">yCacheStorage</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
                                <span class="n">find</span>          <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
                                <span class="k">break</span><span class="p">;</span>
                            <span class="p">}</span>
                        <span class="p">}</span>
                        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">find</span><span class="p">)</span> <span class="p">{</span>
                            <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">bottomY0</span> <span class="o">=</span> <span class="n">bottomData</span> <span class="o">+</span> <span class="n">yp</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">inW</span> <span class="o">*</span> <span class="mi">4</span><span class="p">;</span>
                            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
                                <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">yUsed</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="p">{</span>
                                    <span class="n">yCache</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>     <span class="o">=</span> <span class="n">yp</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
                                    <span class="n">yUsed</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>      <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
                                    <span class="n">yCacheLine</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">yCacheStorage</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
                                    <span class="n">CPUBilinearSampleC4</span><span class="p">(</span><span class="n">bottomY0</span><span class="p">,</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">widthPosition</span><span class="p">,</span> <span class="n">widthFactor</span><span class="p">,</span> <span class="n">outW</span><span class="p">);</span>
                                    <span class="k">break</span><span class="p">;</span>
                                <span class="p">}</span>
                            <span class="p">}</span>
                        <span class="p">}</span>
                    <span class="p">}</span>
                    <span class="k">auto</span> <span class="n">topY</span> <span class="o">=</span> <span class="n">topData</span> <span class="o">+</span> <span class="n">outW</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">dy</span><span class="p">;</span>
                    <span class="c1">// Sample Input
</span><span class="c1"></span>                    <span class="n">CPUBilinearLineC4</span><span class="p">(</span><span class="n">topY</span><span class="p">,</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">heightFactor</span><span class="p">[</span><span class="n">dy</span><span class="p">],</span> <span class="n">outW</span><span class="p">);</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">};</span>
        <span class="n">MNN_CONCURRENCY_BEGIN</span><span class="p">(</span><span class="n">tId</span><span class="p">,</span> <span class="n">threadNumber</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">threadFunction</span><span class="p">(</span><span class="n">tId</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">MNN_CONCURRENCY_END</span><span class="p">();</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">CPUResizeCommon</span><span class="o">::</span><span class="n">CPUResizeCubicC4</span><span class="p">(</span><span class="n">halide_buffer_t</span><span class="o">&amp;</span> <span class="n">input</span><span class="p">,</span> <span class="n">halide_buffer_t</span><span class="o">&amp;</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">batches</span>      <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">inBatchSize</span>  <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">stride</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">outBatchSize</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">stride</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">inW</span>          <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">inH</span>          <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span>            <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">outW</span>         <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">outH</span>         <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">depthQuad</span>    <span class="o">=</span> <span class="n">UP_DIV</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>

    <span class="n">AutoStorage</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">linePosition</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span><span class="p">);</span>
    <span class="n">AutoStorage</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lineFactor</span><span class="p">(</span><span class="n">outW</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">_linePosition</span> <span class="o">=</span> <span class="n">linePosition</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
    <span class="k">auto</span> <span class="n">_lineFactor</span>   <span class="o">=</span> <span class="n">lineFactor</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>

    <span class="c1">// Compute Line Position
</span><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">dx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">dx</span> <span class="o">&lt;</span> <span class="n">outW</span><span class="p">;</span> <span class="o">++</span><span class="n">dx</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">u</span>                   <span class="o">=</span> <span class="p">((</span><span class="kt">float</span><span class="p">)</span><span class="n">dx</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="kt">float</span><span class="p">)(</span><span class="n">outW</span> <span class="o">-</span> <span class="mi">1</span><span class="p">));</span>
        <span class="kt">float</span> <span class="n">x</span>                   <span class="o">=</span> <span class="n">u</span> <span class="o">*</span> <span class="n">inW</span> <span class="o">-</span> <span class="mf">0.5f</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">xInt</span>                  <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">x</span><span class="p">;</span>
        <span class="n">_lineFactor</span><span class="p">[</span><span class="n">dx</span><span class="p">]</span>           <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)(</span><span class="n">x</span> <span class="o">-</span> <span class="n">floor</span><span class="p">(</span><span class="n">x</span><span class="p">));</span>
        <span class="n">_linePosition</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">+</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="n">xInt</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inW</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
        <span class="n">_linePosition</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="n">xInt</span> <span class="o">+</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inW</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
        <span class="n">_linePosition</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="n">xInt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inW</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
        <span class="n">_linePosition</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">+</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="n">xInt</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inW</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">batches</span><span class="p">;</span> <span class="o">++</span><span class="n">b</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">MNN_CONCURRENCY_BEGIN</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">depthQuad</span><span class="p">);</span>
        <span class="p">{</span>
            <span class="kt">int</span> <span class="n">yUsed</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>  <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">};</span>
            <span class="kt">int</span> <span class="n">yCache</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">};</span>

            <span class="n">AutoStorage</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lineBuffer</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="n">outW</span><span class="p">);</span>
            <span class="k">auto</span> <span class="n">_lineBuffer</span>              <span class="o">=</span> <span class="n">lineBuffer</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
            <span class="k">auto</span> <span class="n">_line0</span>                   <span class="o">=</span> <span class="n">_lineBuffer</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="mi">0</span><span class="p">;</span>
            <span class="k">auto</span> <span class="n">_line1</span>                   <span class="o">=</span> <span class="n">_lineBuffer</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="mi">1</span><span class="p">;</span>
            <span class="k">auto</span> <span class="n">_line2</span>                   <span class="o">=</span> <span class="n">_lineBuffer</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="mi">2</span><span class="p">;</span>
            <span class="k">auto</span> <span class="n">_line3</span>                   <span class="o">=</span> <span class="n">_lineBuffer</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="mi">3</span><span class="p">;</span>
            <span class="kt">float</span><span class="o">*</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>          <span class="o">=</span> <span class="p">{</span><span class="n">_line0</span><span class="p">,</span> <span class="n">_line1</span><span class="p">,</span> <span class="n">_line2</span><span class="p">,</span> <span class="n">_line3</span><span class="p">};</span>
            <span class="kt">float</span><span class="o">*</span> <span class="k">const</span> <span class="n">yCacheStorage</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">_line0</span><span class="p">,</span> <span class="n">_line1</span><span class="p">,</span> <span class="n">_line2</span><span class="p">,</span> <span class="n">_line3</span><span class="p">};</span>
            <span class="k">auto</span> <span class="n">bottomData</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">.</span><span class="n">host</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">inBatchSize</span> <span class="o">+</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">n</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">inW</span> <span class="o">*</span> <span class="n">inH</span><span class="p">;</span>
            <span class="k">auto</span> <span class="n">topData</span>    <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">host</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">outBatchSize</span> <span class="o">+</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">n</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">outW</span> <span class="o">*</span> <span class="n">outH</span><span class="p">;</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">dy</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">dy</span> <span class="o">&lt;</span> <span class="n">outH</span><span class="p">;</span> <span class="n">dy</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">float</span> <span class="n">v</span>  <span class="o">=</span> <span class="p">((</span><span class="kt">float</span><span class="p">)</span><span class="n">dy</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="kt">float</span><span class="p">)(</span><span class="n">outH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">));</span>
                <span class="kt">float</span> <span class="n">y</span>  <span class="o">=</span> <span class="n">v</span> <span class="o">*</span> <span class="n">inH</span> <span class="o">-</span> <span class="mf">0.5f</span><span class="p">;</span>
                <span class="kt">int</span> <span class="n">yInt</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">y</span><span class="p">;</span>
                <span class="kt">int</span> <span class="n">yp</span><span class="p">[</span><span class="mi">4</span><span class="p">];</span>
                <span class="n">yp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="n">yInt</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
                <span class="n">yp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="n">yInt</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
                <span class="n">yp</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="n">yInt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
                <span class="n">yp</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLAMP</span><span class="p">(</span><span class="n">yInt</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
                <span class="c1">// Search cache
</span><span class="c1"></span>                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
                    <span class="n">yUsed</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
                    <span class="kt">int</span> <span class="n">find</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
                        <span class="k">if</span> <span class="p">(</span><span class="n">yp</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">yCache</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="p">{</span>
                            <span class="n">yUsed</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>      <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
                            <span class="n">yCacheLine</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">yCacheStorage</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
                            <span class="n">find</span>          <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
                            <span class="k">break</span><span class="p">;</span>
                        <span class="p">}</span>
                    <span class="p">}</span>
                    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">find</span><span class="p">)</span> <span class="p">{</span>
                        <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">bottomY0</span> <span class="o">=</span> <span class="n">bottomData</span> <span class="o">+</span> <span class="n">yp</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">inW</span> <span class="o">*</span> <span class="mi">4</span><span class="p">;</span>
                        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
                            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">yUsed</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="p">{</span>
                                <span class="n">yCache</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>     <span class="o">=</span> <span class="n">yp</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
                                <span class="n">yUsed</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>      <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
                                <span class="n">yCacheLine</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">yCacheStorage</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
                                <span class="n">MNNCubicSampleC4</span><span class="p">(</span><span class="n">bottomY0</span><span class="p">,</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">_linePosition</span><span class="p">,</span> <span class="n">_lineFactor</span><span class="p">,</span> <span class="n">outW</span><span class="p">);</span>
                                <span class="k">break</span><span class="p">;</span>
                            <span class="p">}</span>
                        <span class="p">}</span>
                    <span class="p">}</span>
                <span class="p">}</span>

                <span class="c1">// Sample Input
</span><span class="c1"></span>                <span class="kt">float</span> <span class="n">yFract</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)(</span><span class="n">y</span> <span class="o">-</span> <span class="n">floor</span><span class="p">(</span><span class="n">y</span><span class="p">));</span>
                <span class="k">auto</span> <span class="n">topY</span>    <span class="o">=</span> <span class="n">topData</span> <span class="o">+</span> <span class="n">outW</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">dy</span><span class="p">;</span>
                <span class="n">MNNCubicLineC4</span><span class="p">(</span><span class="n">topY</span><span class="p">,</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">yCacheLine</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">yFract</span><span class="p">,</span> <span class="n">outW</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="n">MNN_CONCURRENCY_END</span><span class="p">();</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><h1 id="broadcast分析">broadcast分析</h1>
<h2 id="概念">概念</h2>
<p>对于shape匹配的tensor，运算可以自动扩展到每个元素，例如下面的操作</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="o">+</span><span class="n">y</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
<span class="p">[[</span> <span class="mi">2</span>  <span class="mi">2</span>  <span class="mi">4</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">5</span>  <span class="mi">5</span>  <span class="mi">7</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">8</span>  <span class="mi">8</span> <span class="mi">10</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>
</code></pre></td></tr></table>
</div>
</div><p>广播允许我们执行隐藏的功能，这使代码更简单，并且提高了内存的使用效率，因为我们不需要再使用其他的操作。</p>
<h2 id="机制">机制</h2>
<p>Broadcasting 机制的核心思想是普适性，即同一份数据能普遍适合于其他位置。在验证普适性之前，需要先将张量shape 靠右对齐，然后进行普适性判断：对于长度为1 的维度，默认这个数据普遍适合于当前维度的其他位置；对于不存在的维度，则在增加新维度后默认当前数据也是普适于新维度的，从而可以扩展为更多维度数、任意长度的张量形状。</p>
<p><img src="/post/image/broadcast.png" alt="image-20200113101343128"></p>
<h2 id="converter实现">converter实现</h2>
<p>converter将tf，caffe模型转换为mnn模型，不涉及计算，未发现广播相关代码</p>
<h2 id="interpreter实现">interpreter实现</h2>
<p>framework通过SizeComputer::computeOutputSize调用算子op的onComputeSize（）函数更新shape信息</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"><span class="kt">bool</span> <span class="n">SizeComputer</span><span class="o">::</span><span class="n">computeOutputSize</span><span class="p">(</span><span class="k">const</span> <span class="n">MNN</span><span class="o">::</span><span class="n">Op</span><span class="o">*</span> <span class="n">op</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">*&gt;&amp;</span> <span class="n">inputs</span><span class="p">,</span>
                                     <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">*&gt;&amp;</span> <span class="n">outputs</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">computeFactory</span> <span class="o">=</span> <span class="n">SizeComputerSuite</span><span class="o">::</span><span class="n">get</span><span class="p">();</span>
    <span class="c1">// When op is nullptr, it means a copy op
</span><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="k">nullptr</span> <span class="o">!=</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">auto</span> <span class="n">computer</span> <span class="o">=</span> <span class="n">computeFactory</span><span class="o">-&gt;</span><span class="n">search</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">());</span>
        <span class="k">if</span> <span class="p">(</span><span class="k">nullptr</span> <span class="o">!=</span> <span class="n">computer</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">bool</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">computer</span><span class="o">-&gt;</span><span class="n">onComputeSize</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">);</span>
            <span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1">// Default Set to the same
</span><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">{</span>
            <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">ib</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">buffer</span><span class="p">();</span>
        <span class="k">auto</span><span class="o">&amp;</span> <span class="n">ob</span>       <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">buffer</span><span class="p">();</span>
        <span class="n">memcpy</span><span class="p">(</span><span class="n">ob</span><span class="p">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">ib</span><span class="p">.</span><span class="n">dim</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">halide_dimension_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">ib</span><span class="p">.</span><span class="n">dimensions</span><span class="p">);</span>
        <span class="n">ob</span><span class="p">.</span><span class="n">dimensions</span>                                         <span class="o">=</span> <span class="n">ib</span><span class="p">.</span><span class="n">dimensions</span><span class="p">;</span>
        <span class="n">ob</span><span class="p">.</span><span class="n">type</span>                                               <span class="o">=</span> <span class="n">ib</span><span class="p">.</span><span class="n">type</span><span class="p">;</span>
        <span class="n">TensorUtils</span><span class="o">::</span><span class="n">getDescribe</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">-&gt;</span><span class="n">dimensionFormat</span> <span class="o">=</span> <span class="n">TensorUtils</span><span class="o">::</span><span class="n">getDescribe</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">-&gt;</span><span class="n">dimensionFormat</span><span class="p">;</span>
        <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1">// Not Support
</span><span class="c1"></span>    <span class="n">MNN_PRINT</span><span class="p">(</span><span class="s">&#34;Can&#39;t compute size for %d, name=%s</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">c_str</span><span class="p">());</span>

    <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>以BinaryOpComputer为例，它支持2个输入tensor的加，减，极大，极小之类的运算</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">Binary
Inputs:
• input0: float32|int32
• input1: float32|int32
Outputs:
• output: float32|int32

Tensorflow op:
• Mul
• Sub
• Add
• Maximum
• RealDiv
• Minimum
• Greater
• BiasAdd
</code></pre></td></tr></table>
</div>
</div><h3 id="cpu-backend">CPU backend</h3>
<p>查看BinaryOpComputer::onComputeSize()函数，里面有对输入tensor的shape检查，如果2者维度不一样，将尽可能的做broadcast</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"><span class="k">class</span> <span class="nc">BinaryOpComputer</span> <span class="o">:</span> <span class="k">public</span> <span class="n">SizeComputer</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
    <span class="k">static</span> <span class="kt">bool</span> <span class="n">outputBool</span><span class="p">(</span><span class="kt">int</span> <span class="n">operation</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">operation</span> <span class="o">==</span> <span class="n">BinaryOpOperation_GREATER_EQUAL</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">operation</span> <span class="o">==</span> <span class="n">BinaryOpOperation_GREATER</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">operation</span> <span class="o">==</span> <span class="n">BinaryOpOperation_LESS</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">operation</span> <span class="o">==</span> <span class="n">BinaryOpOperation_LESS_EQUAL</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">operation</span> <span class="o">==</span> <span class="n">BinaryOpOperation_EQUAL</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="k">virtual</span> <span class="kt">bool</span> <span class="nf">onComputeSize</span><span class="p">(</span><span class="k">const</span> <span class="n">Op</span><span class="o">*</span> <span class="n">op</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">*&gt;&amp;</span> <span class="n">inputs</span><span class="p">,</span>
                               <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">*&gt;&amp;</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
        <span class="n">MNN_ASSERT</span><span class="p">(</span><span class="mi">2</span> <span class="o">==</span> <span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
        <span class="n">MNN_ASSERT</span><span class="p">(</span><span class="mi">1</span> <span class="o">==</span> <span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
        <span class="c1">// set output type &amp; format
</span><span class="c1"></span>        <span class="k">auto</span> <span class="n">input0</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input1</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
        <span class="k">auto</span> <span class="o">&amp;</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">output</span><span class="o">-&gt;</span><span class="n">buffer</span><span class="p">();</span>
        <span class="k">const</span> <span class="k">auto</span> <span class="n">opType</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">main_as_BinaryOp</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">opType</span><span class="p">();</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">outputBool</span><span class="p">(</span><span class="n">opType</span><span class="p">))</span> <span class="p">{</span>
            <span class="n">buffer</span><span class="p">.</span><span class="n">type</span> <span class="o">=</span> <span class="n">halide_type_of</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">();</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">buffer</span><span class="p">.</span><span class="n">type</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">getType</span><span class="p">();</span>
        <span class="p">}</span>
        <span class="n">TensorUtils</span><span class="o">::</span><span class="n">getDescribe</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">dimensionFormat</span> <span class="o">=</span> <span class="n">TensorUtils</span><span class="o">::</span><span class="n">getDescribe</span><span class="p">(</span><span class="n">input0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">dimensionFormat</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">input0</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">())</span> <span class="p">{</span>
            <span class="k">auto</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">input0</span><span class="p">;</span>
            <span class="n">input0</span> <span class="o">=</span> <span class="n">input1</span><span class="p">;</span>
            <span class="n">input1</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// if scalar input -&gt; just copy the other
</span><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">input1</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">TensorUtils</span><span class="o">::</span><span class="n">copyShape</span><span class="p">(</span><span class="n">input0</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// else if inputs shape equals -&gt; just copy any one
</span><span class="c1"></span>        <span class="kt">bool</span> <span class="n">sameShape</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">()</span> <span class="o">==</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">sameShape</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">TensorUtils</span><span class="o">::</span><span class="n">copyShape</span><span class="p">(</span><span class="n">input0</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// else if broadcast NOT supported -&gt; failed
</span><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">maxDimensions</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">();</span> 
        <span class="k">const</span> <span class="kt">int</span> <span class="n">diffDimension</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">()</span> <span class="o">-</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">();</span>

        <span class="c1">// else broadcast
</span><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">maxDimensions</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span><span class="mi">0</span> <span class="p">;</span> <span class="o">--</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">auto</span> <span class="n">input0Length</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
            <span class="k">auto</span> <span class="n">input1Length</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="n">diffDimension</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">input1Length</span> <span class="o">=</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">diffDimension</span><span class="p">);</span>
            <span class="p">}</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">input0Length</span> <span class="o">!=</span> <span class="n">input1Length</span> <span class="o">&amp;&amp;</span> <span class="n">input1Length</span> <span class="o">!=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">input0Length</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">MNN_PRINT</span><span class="p">(</span><span class="s">&#34;%d, %d</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="n">input1Length</span><span class="p">,</span> <span class="n">input0Length</span><span class="p">);</span>
                <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
            <span class="p">}</span>
            <span class="n">buffer</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">extent</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">input0Length</span><span class="p">,</span> <span class="n">input1Length</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">buffer</span><span class="p">.</span><span class="n">dimensions</span> <span class="o">=</span> <span class="n">maxDimensions</span><span class="p">;</span>
        <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">};</span>

<span class="n">REGISTER_SHAPE</span><span class="p">(</span><span class="n">BinaryOpComputer</span><span class="p">,</span> <span class="n">OpType_BinaryOp</span><span class="p">);</span>
</code></pre></td></tr></table>
</div>
</div><p>onComputeSize（）函数首先设置输出变量的类型，再检查两个输入的维度，如果input0比较小的话，交换两个输入以方便后续代码</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">if (input0-&gt;dimensions() &lt; input1-&gt;dimensions()) {
            auto temp = input0;
            input0 = input1;
            input1 = temp;
        }
</code></pre></td></tr></table>
</div>
</div><p>如果输入变量维度不一致，首先计算维度的最大值和差异值</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"><span class="k">const</span> <span class="kt">int</span> <span class="n">maxDimensions</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">();</span> 
<span class="k">const</span> <span class="kt">int</span> <span class="n">diffDimension</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">()</span> <span class="o">-</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">();</span>
</code></pre></td></tr></table>
</div>
</div><p>接下来一个循环，计算可以broadcast的维度，与之前机制中的算法相同</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">maxDimensions</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span><span class="mi">0</span> <span class="p">;</span> <span class="o">--</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">auto</span> <span class="n">input0Length</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="c1">// 输入0的第i位维度
</span><span class="c1"></span>            <span class="k">auto</span> <span class="n">input1Length</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>                <span class="c1">// 输入1的第i位维度默认为1
</span><span class="c1"></span>            <span class="c1">// input共有部分维度
</span><span class="c1"></span>            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="n">diffDimension</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">input1Length</span> <span class="o">=</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">diffDimension</span><span class="p">);</span> <span class="c1">// 取输入1的第i位维度
</span><span class="c1"></span>            <span class="p">}</span>
            <span class="c1">// 二者维度不一致且不是任一个维度为1，这种情况无法做broadcast
</span><span class="c1"></span>            <span class="k">if</span> <span class="p">(</span><span class="n">input0Length</span> <span class="o">!=</span> <span class="n">input1Length</span> <span class="o">&amp;&amp;</span> <span class="n">input1Length</span> <span class="o">!=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">input0Length</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">MNN_PRINT</span><span class="p">(</span><span class="s">&#34;%d, %d</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="n">input1Length</span><span class="p">,</span> <span class="n">input0Length</span><span class="p">);</span>
                <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
            <span class="p">}</span>
            <span class="c1">// 修正两个输入第i位维度值大的那个，完成维度计算
</span><span class="c1"></span>            <span class="n">buffer</span><span class="p">.</span><span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">extent</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">input0Length</span><span class="p">,</span> <span class="n">input1Length</span><span class="p">);</span>
        <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>最后，将输出buffer的维度调整为最大值</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">buffer.dimensions = maxDimensions;
</code></pre></td></tr></table>
</div>
</div><p>然后再看具体的计算过程 source/backend/cpu/CPUBinary.cpp</p>
<p>算子onExecute（）函数根据传入的计算类型，调用相应模板完成计算</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
<span class="n">ErrorCode</span> <span class="n">CPUBinary</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;::</span><span class="n">onExecute</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">*&gt;&amp;</span> <span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">*&gt;&amp;</span> <span class="n">outputs</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="k">nullptr</span> <span class="o">!=</span> <span class="n">mEltWise</span><span class="p">.</span><span class="n">get</span><span class="p">())</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">mEltWise</span><span class="o">-&gt;</span><span class="n">onExecute</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">auto</span> <span class="n">input</span>  <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
    <span class="k">auto</span> <span class="n">input1</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
    <span class="k">auto</span> <span class="n">output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>

    <span class="k">switch</span> <span class="p">(</span><span class="n">mType</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_MUL</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryMul</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_ADD</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryAdd</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_SUB</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinarySub</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>

        <span class="k">case</span> <span class="nl">BinaryOpOperation_REALDIV</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryRealDiv</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_MINIMUM</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryMin</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_MAXIMUM</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryMax</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_GREATER</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="kt">int32_t</span><span class="p">,</span> <span class="n">BinaryGreater</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="kt">int32_t</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_LESS</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryLess</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="kt">int32_t</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_LESS_EQUAL</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryLessEqual</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="kt">int32_t</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_GREATER_EQUAL</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryGreaterEqual</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="kt">int32_t</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_EQUAL</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryEqual</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="kt">int32_t</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_FLOORDIV</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryFloorDiv</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_FLOORMOD</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryFloorMod</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_POW</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinaryPow</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">case</span> <span class="nl">BinaryOpOperation_SquaredDifference</span><span class="p">:</span>
            <span class="n">_binaryOp</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">BinarySquaredDifference</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="k">default</span><span class="o">:</span>
            <span class="n">MNN_ASSERT</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
            <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">NO_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>上述计算的核心是_binaryOp（）函数，发现不是相同类型的输入，那么根据之前计算的输出tensor shape参数循环计算结果。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span><span class="lnt">90
</span><span class="lnt">91
</span><span class="lnt">92
</span><span class="lnt">93
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Tin</span><span class="p">,</span> <span class="k">typename</span> <span class="n">Tout</span><span class="p">,</span> <span class="k">typename</span> <span class="n">Func</span><span class="o">&gt;</span>
<span class="k">static</span> <span class="n">ErrorCode</span> <span class="n">_binaryOp</span><span class="p">(</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">input0</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">*</span> <span class="n">input1</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">Func</span> <span class="n">f</span><span class="p">;</span>

    <span class="k">const</span> <span class="kt">int</span> <span class="n">input0DataCount</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">input1DataCount</span> <span class="o">=</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>

    <span class="k">const</span> <span class="n">Tin</span><span class="o">*</span> <span class="n">input0Data</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="n">Tin</span><span class="o">&gt;</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">Tin</span><span class="o">*</span> <span class="n">input1Data</span> <span class="o">=</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="n">Tin</span><span class="o">&gt;</span><span class="p">();</span>
    <span class="n">Tout</span><span class="o">*</span> <span class="n">outputData</span>      <span class="o">=</span> <span class="n">output</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="n">Tout</span><span class="o">&gt;</span><span class="p">();</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">input0DataCount</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// data count == 1, not only mean scalar input, maybe of shape (1, 1, 1, ...,1)
</span><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">input1DataCount</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">outputData</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">Tout</span><span class="o">&gt;</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">input0Data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input1Data</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span>
        <span class="p">}</span>
    <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">input1DataCount</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">input0DataCount</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">outputData</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">Tout</span><span class="o">&gt;</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">input0Data</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">input1Data</span><span class="p">[</span><span class="mi">0</span><span class="p">]));</span>
        <span class="p">}</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span> <span class="c1">// both input contains more than one element，which means no scalar input
</span><span class="c1"></span>        <span class="kt">bool</span> <span class="n">sameShape</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">()</span> <span class="o">==</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">sameShape</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// two inputs have the same shape, apply element-wise operation
</span><span class="c1"></span>            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">input0DataCount</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">outputData</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">Tout</span><span class="o">&gt;</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">input0Data</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">input1Data</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span>
            <span class="p">}</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span> <span class="c1">// not the same shape, use broadcast
</span><span class="c1"></span><span class="cp">#define MAX_DIM 6
</span><span class="cp"></span>            <span class="c1">// 输入shape类型不一样，根据之前计算的output-&gt;dimensions完成broadcast计算
</span><span class="c1"></span>            <span class="n">MNN_ASSERT</span><span class="p">(</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">MAX_DIM</span><span class="p">);</span>
            <span class="kt">int</span> <span class="n">dims</span><span class="p">[</span><span class="n">MAX_DIM</span><span class="p">];</span>
            <span class="kt">int</span> <span class="n">stride</span><span class="p">[</span><span class="n">MAX_DIM</span><span class="p">];</span>
            <span class="kt">int</span> <span class="n">iStride0</span><span class="p">[</span><span class="n">MAX_DIM</span><span class="p">];</span>
            <span class="kt">int</span> <span class="n">iStride1</span><span class="p">[</span><span class="n">MAX_DIM</span><span class="p">];</span>
            <span class="c1">// 更新输入，输出tensor最多6维对应的dim，strde信息
</span><span class="c1"></span>            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">MAX_DIM</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="o">--</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
                <span class="c1">// dim， stride默认值
</span><span class="c1"></span>                <span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>     <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
                <span class="n">stride</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>   <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="n">iStride0</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="n">iStride1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="c1">// 输入索引
</span><span class="c1"></span>                <span class="kt">int</span> <span class="n">input0I</span> <span class="o">=</span> <span class="n">i</span> <span class="o">-</span> <span class="p">(</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">()</span> <span class="o">-</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">());</span>
                <span class="kt">int</span> <span class="n">input1I</span> <span class="o">=</span> <span class="n">i</span> <span class="o">-</span> <span class="p">(</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">()</span> <span class="o">-</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">());</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">output</span><span class="o">-&gt;</span><span class="n">dimensions</span><span class="p">())</span> <span class="p">{</span>
                    <span class="c1">// 已有数据，填实际的dim，stride
</span><span class="c1"></span>                    <span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>   <span class="o">=</span> <span class="n">output</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
                    <span class="n">stride</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">output</span><span class="o">-&gt;</span><span class="n">stride</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
                <span class="p">}</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">input0I</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">(</span><span class="n">input0I</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
                    <span class="c1">// 原有数据维度，填实际stride，否则stride用默认值0，重复使用现有数据
</span><span class="c1"></span>                    <span class="n">iStride0</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">input0</span><span class="o">-&gt;</span><span class="n">stride</span><span class="p">(</span><span class="n">input0I</span><span class="p">);</span>
                <span class="p">}</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">input1I</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">(</span><span class="n">input1I</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
                    <span class="c1">// 原有数据维度，填实际stride，否则stride用默认值0，重复使用现有数据
</span><span class="c1"></span>                    <span class="n">iStride1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">input1</span><span class="o">-&gt;</span><span class="n">stride</span><span class="p">(</span><span class="n">input1I</span><span class="p">);</span>
                <span class="p">}</span>
            <span class="p">}</span>
            <span class="c1">// 根据“索引地址=首地址 + x * stride[i]循环计算6维结果”
</span><span class="c1"></span>            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">w</span> <span class="o">&lt;</span> <span class="n">dims</span><span class="p">[</span><span class="mi">5</span><span class="p">];</span> <span class="o">++</span><span class="n">w</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">auto</span> <span class="n">ow</span>  <span class="o">=</span> <span class="n">outputData</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="mi">5</span><span class="p">];</span>
                <span class="k">auto</span> <span class="n">i0w</span> <span class="o">=</span> <span class="n">input0Data</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">iStride0</span><span class="p">[</span><span class="mi">5</span><span class="p">];</span>
                <span class="k">auto</span> <span class="n">i1w</span> <span class="o">=</span> <span class="n">input1Data</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">iStride1</span><span class="p">[</span><span class="mi">5</span><span class="p">];</span>
<span class="cp">#define PTR(x, y, i)                      \
</span><span class="cp">    auto o##x  = o##y + x * stride[i];    \
</span><span class="cp">    auto i0##x = i0##y + x * iStride0[i]; \
</span><span class="cp">    auto i1##x = i1##y + x * iStride1[i]
</span><span class="cp"></span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">v</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">dims</span><span class="p">[</span><span class="mi">4</span><span class="p">];</span> <span class="o">++</span><span class="n">v</span><span class="p">)</span> <span class="p">{</span>
                    <span class="n">PTR</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>
                    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">u</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">u</span> <span class="o">&lt;</span> <span class="n">dims</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span> <span class="o">++</span><span class="n">u</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">PTR</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="mi">3</span><span class="p">);</span>
                        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">z</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">z</span> <span class="o">&lt;</span> <span class="n">dims</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span> <span class="o">++</span><span class="n">z</span><span class="p">)</span> <span class="p">{</span>
                            <span class="n">PTR</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
                            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span> <span class="o">++</span><span class="n">y</span><span class="p">)</span> <span class="p">{</span>
                                <span class="n">PTR</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
                                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="o">++</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
                                    <span class="n">PTR</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
                                    <span class="c1">// 此处对应一个原子操作
</span><span class="c1"></span>                                    <span class="o">*</span><span class="n">ox</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">Tout</span><span class="o">&gt;</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">i0x</span><span class="p">,</span> <span class="o">*</span><span class="n">i1x</span><span class="p">));</span>
                                <span class="p">}</span>
                            <span class="p">}</span>
                        <span class="p">}</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>
<span class="cp">#undef MAX_DIM
</span><span class="cp">#undef PTR
</span><span class="cp"></span>        <span class="p">}</span>
        <span class="c1">// broadcast-capable check is done in compute size
</span><span class="c1"></span>    <span class="p">}</span>

    <span class="k">return</span> <span class="n">NO_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p><!-- raw HTML omitted -->此处最多循环6维，最外面几层循环次数为1。为什么最大为6？？岂不是大于6维的输入会没法计算？<!-- raw HTML omitted --></p>
<p><!-- raw HTML omitted -->之前拿到的mnn版本为0.2.1.4， 需要check最新的0.2.1.6版本，可能增加了一些支持broadcast的算子<!-- raw HTML omitted --></p>
<h3 id="metal-backend">metal backend</h3>
<p>看source/backend/metal/MetalBinary.mm里面的MetalBinary::onExecute（）函数，如果shape不一致，</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-swift" data-lang="swift"><span class="n">ErrorCode</span> <span class="n">MetalBinary</span><span class="p">::</span><span class="n">onExecute</span><span class="p">(</span><span class="n">const</span> <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="p">&lt;</span><span class="n">Tensor</span> <span class="o">*&gt;</span> <span class="p">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="n">const</span> <span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="p">&lt;</span><span class="n">Tensor</span> <span class="o">*&gt;</span> <span class="p">&amp;</span><span class="n">outputs</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">auto</span> <span class="n">backend</span> <span class="p">=</span> <span class="n">static_cast</span><span class="p">&lt;</span><span class="n">MetalBackend</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">this</span><span class="p">-&gt;</span><span class="n">backend</span><span class="p">());</span>
    <span class="n">auto</span> <span class="n">context</span> <span class="p">=</span> <span class="p">(</span><span class="n">__bridge</span> <span class="n">MNNMetalContext</span> <span class="o">*</span><span class="p">)</span><span class="n">backend</span><span class="p">-&gt;</span><span class="n">context</span><span class="p">();</span>
    <span class="n">auto</span> <span class="n">input0</span> <span class="p">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input1</span> <span class="p">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output</span> <span class="p">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
    <span class="n">const</span> <span class="n">int</span> <span class="n">input0_data_count</span> <span class="p">=</span> <span class="p">(</span><span class="n">int</span><span class="p">)</span><span class="n">input0</span><span class="p">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>
    <span class="n">const</span> <span class="n">int</span> <span class="n">input1_data_count</span> <span class="p">=</span> <span class="p">(</span><span class="n">int</span><span class="p">)</span><span class="n">input1</span><span class="p">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>

    <span class="c1">// scalar support</span>
    <span class="n">int</span> <span class="n">iw0</span> <span class="p">=</span> <span class="n">input0</span><span class="p">-&gt;</span><span class="n">width</span><span class="p">(),</span> <span class="n">ih0</span> <span class="p">=</span> <span class="n">input0</span><span class="p">-&gt;</span><span class="n">height</span><span class="p">();</span>
    <span class="n">int</span> <span class="n">iw1</span> <span class="p">=</span> <span class="n">input1</span><span class="p">-&gt;</span><span class="n">width</span><span class="p">(),</span> <span class="n">ih1</span> <span class="p">=</span> <span class="n">input1</span><span class="p">-&gt;</span><span class="n">height</span><span class="p">();</span>
    <span class="n">int</span> <span class="n">ow</span> <span class="p">=</span> <span class="n">output</span><span class="p">-&gt;</span><span class="n">width</span><span class="p">(),</span> <span class="n">oh</span> <span class="p">=</span> <span class="n">output</span><span class="p">-&gt;</span><span class="n">height</span><span class="p">(),</span> <span class="n">oc</span> <span class="p">=</span> <span class="n">output</span><span class="p">-&gt;</span><span class="n">channel</span><span class="p">(),</span> <span class="n">ob</span> <span class="p">=</span> <span class="n">output</span><span class="p">-&gt;</span><span class="n">batch</span><span class="p">();</span>
    <span class="n">iw0</span> <span class="p">=</span> <span class="n">iw0</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">?</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">iw0</span><span class="p">;</span>
    <span class="n">ih0</span> <span class="p">=</span> <span class="n">ih0</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">?</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">ih0</span><span class="p">;</span>
    <span class="n">iw1</span> <span class="p">=</span> <span class="n">iw1</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">?</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">iw1</span><span class="p">;</span>
    <span class="n">ih1</span> <span class="p">=</span> <span class="n">ih1</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">?</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">ih1</span><span class="p">;</span>
    <span class="n">ow</span>  <span class="p">=</span> <span class="n">ow</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">?</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">ow</span><span class="p">;</span>
    <span class="n">oh</span>  <span class="p">=</span> <span class="n">oh</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">?</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">oh</span><span class="p">;</span>
    <span class="n">oc</span>  <span class="p">=</span> <span class="n">oc</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">?</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">oc</span><span class="p">;</span>
    <span class="n">ob</span>  <span class="p">=</span> <span class="n">ob</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">?</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">ob</span><span class="p">;</span>

    <span class="n">bool</span> <span class="n">same_shape</span> <span class="p">=</span> <span class="kc">true</span><span class="p">;</span>
    <span class="c1">// scalar input</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dimensions</span> <span class="p">==</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dimensions</span> <span class="p">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// do nothing</span>
    <span class="p">}</span>
    <span class="c1">// same shape</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dimensions</span> <span class="p">==</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dimensions</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">int</span> <span class="n">i</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dimensions</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">extent</span> <span class="o">!=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">extent</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">same_shape</span> <span class="p">=</span> <span class="kc">false</span><span class="p">;</span>
                <span class="k">break</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="c1">// different shape</span>
    <span class="k">else</span> <span class="p">{</span>
        <span class="n">same_shape</span> <span class="p">=</span> <span class="kc">false</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// encode</span>
    <span class="n">auto</span> <span class="n">output_dimensions</span> <span class="p">=</span> <span class="n">output</span><span class="p">-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dimensions</span><span class="p">;</span>
    <span class="n">auto</span> <span class="n">shape</span>             <span class="p">=</span> <span class="p">[</span><span class="n">context</span> <span class="n">newDeviceBuffer</span><span class="p">:</span><span class="mi">6</span> <span class="o">*</span> <span class="bp">sizeof</span><span class="p">(</span><span class="n">int</span><span class="p">)</span> <span class="n">access</span><span class="p">:</span><span class="n">CPUWriteOnly</span><span class="p">];</span>
    <span class="n">auto</span> <span class="n">encoder</span>           <span class="p">=</span> <span class="p">[</span><span class="n">context</span> <span class="n">encoder</span><span class="p">];</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">same_shape</span> <span class="p">==</span> <span class="kc">false</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// 维度不一致情况，需要计算各个维度的dim，stride，然后才能计算</span>
        <span class="c1">// dim</span>
        <span class="n">auto</span> <span class="n">dimsIn0Buffer</span> <span class="p">=</span> <span class="p">[</span><span class="n">context</span> <span class="n">newDeviceBuffer</span><span class="p">:</span><span class="bp">sizeof</span><span class="p">(</span><span class="n">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">output_dimensions</span> <span class="n">access</span><span class="p">:</span><span class="n">CPUWriteOnly</span><span class="p">];</span>
        <span class="n">auto</span> <span class="n">dimsIn1Buffer</span> <span class="p">=</span> <span class="p">[</span><span class="n">context</span> <span class="n">newDeviceBuffer</span><span class="p">:</span><span class="bp">sizeof</span><span class="p">(</span><span class="n">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">output_dimensions</span> <span class="n">access</span><span class="p">:</span><span class="n">CPUWriteOnly</span><span class="p">];</span>
        <span class="n">int</span> <span class="o">*</span><span class="n">dims0</span>         <span class="p">=</span> <span class="p">(</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">dimsIn0Buffer</span><span class="p">.</span><span class="n">contents</span><span class="p">;</span>
        <span class="n">int</span> <span class="o">*</span><span class="n">dims1</span>         <span class="p">=</span> <span class="p">(</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">dimsIn1Buffer</span><span class="p">.</span><span class="n">contents</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">int</span> <span class="n">i</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">output_dimensions</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">dims0</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">=</span> <span class="n">dims1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">int</span> <span class="n">i</span> <span class="p">=</span> <span class="n">input0</span><span class="p">-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dimensions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span> <span class="p">=</span> <span class="n">output_dimensions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">,</span> <span class="n">j</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">dims0</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="p">=</span> <span class="n">input0</span><span class="p">-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">int</span> <span class="n">i</span> <span class="p">=</span> <span class="n">input1</span><span class="p">-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dimensions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span> <span class="p">=</span> <span class="n">output_dimensions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">,</span> <span class="n">j</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">dims1</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="p">=</span> <span class="n">input1</span><span class="p">-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// strides &amp; shape</span>
        <span class="n">auto</span> <span class="n">stridesIn0Buffer</span> <span class="p">=</span> <span class="p">[</span><span class="n">context</span> <span class="n">newDeviceBuffer</span><span class="p">:</span><span class="bp">sizeof</span><span class="p">(</span><span class="n">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">output_dimensions</span> <span class="n">access</span><span class="p">:</span><span class="n">CPUWriteOnly</span><span class="p">];</span>
        <span class="n">auto</span> <span class="n">stridesIn1Buffer</span> <span class="p">=</span> <span class="p">[</span><span class="n">context</span> <span class="n">newDeviceBuffer</span><span class="p">:</span><span class="bp">sizeof</span><span class="p">(</span><span class="n">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">output_dimensions</span> <span class="n">access</span><span class="p">:</span><span class="n">CPUWriteOnly</span><span class="p">];</span>
        <span class="n">auto</span> <span class="n">stridesOutBuffer</span> <span class="p">=</span> <span class="p">[</span><span class="n">context</span> <span class="n">newDeviceBuffer</span><span class="p">:</span><span class="bp">sizeof</span><span class="p">(</span><span class="n">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">output_dimensions</span> <span class="n">access</span><span class="p">:</span><span class="n">CPUWriteOnly</span><span class="p">];</span>
        <span class="n">int</span> <span class="o">*</span><span class="n">input0_strides</span>   <span class="p">=</span> <span class="p">(</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">stridesIn0Buffer</span><span class="p">.</span><span class="n">contents</span><span class="p">;</span>
        <span class="n">int</span> <span class="o">*</span><span class="n">input1_strides</span>   <span class="p">=</span> <span class="p">(</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">stridesIn1Buffer</span><span class="p">.</span><span class="n">contents</span><span class="p">;</span>
        <span class="n">int</span> <span class="o">*</span><span class="n">output_strides</span>   <span class="p">=</span> <span class="p">(</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">stridesOutBuffer</span><span class="p">.</span><span class="n">contents</span><span class="p">;</span>
        <span class="n">int</span> <span class="n">input_data_count0</span> <span class="p">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">input_data_count1</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
        <span class="n">int</span> <span class="n">output_data_count</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
        <span class="c1">// 更新各个维度的stride信息</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">int</span> <span class="n">i</span> <span class="p">=</span> <span class="n">output_dimensions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">input0_strides</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">=</span> <span class="n">input_data_count0</span><span class="p">;</span>
            <span class="n">input_data_count0</span> <span class="o">*=</span> <span class="n">dims0</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
            <span class="n">input1_strides</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">=</span> <span class="n">input_data_count1</span><span class="p">;</span>
            <span class="n">input_data_count1</span> <span class="o">*=</span> <span class="n">dims1</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
            <span class="n">output_strides</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">=</span> <span class="n">output_data_count</span><span class="p">;</span>
            <span class="n">output_data_count</span> <span class="o">*=</span> <span class="n">output</span><span class="p">-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">extent</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="p">=</span> <span class="n">input0_data_count</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="p">=</span> <span class="n">input1_data_count</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span> <span class="p">=</span> <span class="n">output_data_count</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">3</span><span class="p">]</span> <span class="p">=</span> <span class="n">ow</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">4</span><span class="p">]</span> <span class="p">=</span> <span class="n">ow</span> <span class="o">*</span> <span class="n">oh</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">5</span><span class="p">]</span> <span class="p">=</span> <span class="n">output_dimensions</span><span class="p">;</span>

        <span class="c1">// encode</span>
        <span class="n">auto</span> <span class="n">bandwidth</span> <span class="p">=</span> <span class="p">[</span><span class="n">context</span> <span class="n">load</span><span class="p">:@</span><span class="s">&#34;binary_notshape&#34;</span> <span class="n">encoder</span><span class="p">:</span><span class="n">encoder</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:(</span><span class="n">__bridge</span> <span class="n">id</span><span class="p">&lt;</span><span class="n">MTLBuffer</span><span class="p">&gt;)(</span><span class="n">void</span> <span class="o">*</span><span class="p">)</span><span class="n">input0</span><span class="p">-&gt;</span><span class="n">deviceId</span><span class="p">()</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">0</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:(</span><span class="n">__bridge</span> <span class="n">id</span><span class="p">&lt;</span><span class="n">MTLBuffer</span><span class="p">&gt;)(</span><span class="n">void</span> <span class="o">*</span><span class="p">)</span><span class="n">input1</span><span class="p">-&gt;</span><span class="n">deviceId</span><span class="p">()</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">1</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:(</span><span class="n">__bridge</span> <span class="n">id</span><span class="p">&lt;</span><span class="n">MTLBuffer</span><span class="p">&gt;)(</span><span class="n">void</span> <span class="o">*</span><span class="p">)</span><span class="n">output</span><span class="p">-&gt;</span><span class="n">deviceId</span><span class="p">()</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">2</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:</span><span class="n">shape</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">3</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:</span><span class="n">mBinaryType</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">4</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:</span><span class="n">dimsIn0Buffer</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">5</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:</span><span class="n">dimsIn1Buffer</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">6</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:</span><span class="n">stridesIn0Buffer</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">7</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:</span><span class="n">stridesIn1Buffer</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">8</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:</span><span class="n">stridesOutBuffer</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">9</span><span class="p">];</span>
        <span class="p">[</span><span class="n">context</span> <span class="n">dispatchEncoder</span><span class="p">:</span><span class="n">encoder</span> <span class="n">threads</span><span class="p">:{</span> <span class="p">(</span><span class="n">NSUInteger</span><span class="p">)</span> <span class="n">output_data_count</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="p">}</span> <span class="n">bandwidth</span><span class="p">:</span><span class="n">bandwidth</span><span class="p">];</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">int</span> <span class="n">outdatacount</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">input0_data_count</span> <span class="p">==</span> <span class="n">input1_data_count</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">outdatacount</span> <span class="p">=</span> <span class="n">input0_data_count</span><span class="p">;</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">outdatacount</span> <span class="p">=</span> <span class="n">input0_data_count</span> <span class="o">&gt;</span> <span class="n">input1_data_count</span> <span class="p">?</span> <span class="n">input0_data_count</span> <span class="p">:</span> <span class="n">input1_data_count</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="p">=</span> <span class="n">input0_data_count</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="p">=</span> <span class="n">input1_data_count</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span> <span class="p">=</span> <span class="n">outdatacount</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">3</span><span class="p">]</span> <span class="p">=</span> <span class="n">ow</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">4</span><span class="p">]</span> <span class="p">=</span> <span class="n">ow</span> <span class="o">*</span> <span class="n">oh</span><span class="p">;</span>
        <span class="p">((</span><span class="n">int</span> <span class="o">*</span><span class="p">)</span><span class="n">shape</span><span class="p">.</span><span class="n">contents</span><span class="p">)[</span><span class="mi">5</span><span class="p">]</span> <span class="p">=</span> <span class="n">output_dimensions</span><span class="p">;</span>

        <span class="n">auto</span> <span class="n">bandwidth</span> <span class="p">=</span> <span class="p">[</span><span class="n">context</span> <span class="n">load</span><span class="p">:@</span><span class="s">&#34;binary_normal&#34;</span> <span class="n">encoder</span><span class="p">:</span><span class="n">encoder</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:(</span><span class="n">__bridge</span> <span class="n">id</span><span class="p">&lt;</span><span class="n">MTLBuffer</span><span class="p">&gt;)(</span><span class="n">void</span> <span class="o">*</span><span class="p">)</span><span class="n">input0</span><span class="p">-&gt;</span><span class="n">deviceId</span><span class="p">()</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">0</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:(</span><span class="n">__bridge</span> <span class="n">id</span><span class="p">&lt;</span><span class="n">MTLBuffer</span><span class="p">&gt;)(</span><span class="n">void</span> <span class="o">*</span><span class="p">)</span><span class="n">input1</span><span class="p">-&gt;</span><span class="n">deviceId</span><span class="p">()</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">1</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:(</span><span class="n">__bridge</span> <span class="n">id</span><span class="p">&lt;</span><span class="n">MTLBuffer</span><span class="p">&gt;)(</span><span class="n">void</span> <span class="o">*</span><span class="p">)</span><span class="n">output</span><span class="p">-&gt;</span><span class="n">deviceId</span><span class="p">()</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">2</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:</span><span class="n">shape</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">3</span><span class="p">];</span>
        <span class="p">[</span><span class="n">encoder</span> <span class="n">setBuffer</span><span class="p">:</span><span class="n">mBinaryType</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span> <span class="n">atIndex</span><span class="p">:</span><span class="mi">4</span><span class="p">];</span>
        <span class="p">[</span><span class="n">context</span> <span class="n">dispatchEncoder</span><span class="p">:</span><span class="n">encoder</span> <span class="n">threads</span><span class="p">:{</span> <span class="p">(</span><span class="n">NSUInteger</span><span class="p">)</span> <span class="n">outdatacount</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="p">}</span> <span class="n">bandwidth</span><span class="p">:</span><span class="n">bandwidth</span><span class="p">];</span>
    <span class="p">}</span>

    <span class="p">[</span><span class="n">encoder</span> <span class="n">endEncoding</span><span class="p">];</span>
    <span class="n">MNN_PRINT_ENCODER</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">encoder</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">NO_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="vulkan-backend">vulkan backend</h3>
<p>vulkan后端不支持broadcast，只是为NHWC和NC4HW4 做了优化</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">ErrorCode VulkanBinary::onEncode(const std::vector&lt;Tensor*&gt;&amp; inputs, const std::vector&lt;Tensor*&gt;&amp; outputs,
                                 const VulkanCommandPool::Buffer* cmdBuffer) {
    MNN_ASSERT(2 == inputs.size());
    MNN_ASSERT(1 == outputs.size());

    auto input0 = inputs[0];
    auto input1 = inputs[1];
    auto output = outputs[0];
    MNN_ASSERT(input0-&gt;getType().code == halide_type_float);
    const auto intputFormat = TensorUtils::getDescribe(input0)-&gt;dimensionFormat;
    if (intputFormat == MNN_DATA_FORMAT_NHWC) {
        // for NHWC input
        std::vector&lt;VkDescriptorType&gt; types{VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                                            VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER};

        switch (mType) {
            case BinaryOpOperation_MUL:
                mBinaryPipeline = mVkBackend-&gt;getPipeline(&#34;glsl_elementwiseMulBuffer_comp&#34;, types);
                break;
            case BinaryOpOperation_ADD:
                mBinaryPipeline = mVkBackend-&gt;getPipeline(&#34;glsl_elementwiseAddBuffer_comp&#34;, types);
                break;
            case BinaryOpOperation_SUB:
                mBinaryPipeline = mVkBackend-&gt;getPipeline(&#34;glsl_elementwiseSubBuffer_comp&#34;, types);
                break;
            default:
                MNN_PRINT(&#34;Not Supported Binary Operation: %d\n&#34;, mType);
                MNN_ASSERT(false);
                break;
        }

        const int input0Elements = input0-&gt;elementSize();
        const int input1Elements = input1-&gt;elementSize();
        const int outputElements = output-&gt;elementSize();

        auto binaryOpParam = reinterpret_cast&lt;ConstBuffer*&gt;(mConstBuffer-&gt;map());
        ::memset(binaryOpParam, 0, sizeof(ConstBuffer));

        if (input0Elements == 1) {
            binaryOpParam-&gt;stride[0] = 0;
            binaryOpParam-&gt;stride[1] = 1;
            binaryOpParam-&gt;stride[2] = 1;
            binaryOpParam-&gt;stride[3] = outputElements;
        } else if (input1Elements == 1) {
            binaryOpParam-&gt;stride[0] = 1;
            binaryOpParam-&gt;stride[1] = 0;
            binaryOpParam-&gt;stride[2] = 1;
            binaryOpParam-&gt;stride[3] = outputElements;
        } else if (input0Elements == input1Elements) {
            binaryOpParam-&gt;stride[0] = 1;
            binaryOpParam-&gt;stride[1] = 1;
            binaryOpParam-&gt;stride[2] = 1;
            binaryOpParam-&gt;stride[3] = outputElements;
        } else {
            return NOT_SUPPORT;
        }
        mConstBuffer-&gt;flush(true, 0, sizeof(ConstBuffer));
        mConstBuffer-&gt;unmap();

        mDescriptorSet.reset(mBinaryPipeline-&gt;createSet());
        mDescriptorSet-&gt;writeBuffer(reinterpret_cast&lt;VkBuffer&gt;(output-&gt;deviceId()), 0, output-&gt;size());
        mDescriptorSet-&gt;writeBuffer(reinterpret_cast&lt;VkBuffer&gt;(input0-&gt;deviceId()), 1, input0-&gt;size());
        mDescriptorSet-&gt;writeBuffer(reinterpret_cast&lt;VkBuffer&gt;(input1-&gt;deviceId()), 2, input1-&gt;size());
        mDescriptorSet-&gt;writeBuffer(mConstBuffer-&gt;buffer(), 3, mConstBuffer-&gt;size());
        mBinaryPipeline-&gt;bind(cmdBuffer-&gt;get(), mDescriptorSet-&gt;get());
        cmdBuffer-&gt;barrierSource(reinterpret_cast&lt;VkBuffer&gt;(input0-&gt;deviceId()), 0, input0-&gt;size());
        cmdBuffer-&gt;barrierSource(reinterpret_cast&lt;VkBuffer&gt;(input1-&gt;deviceId()), 0, input1-&gt;size());
        vkCmdDispatch(cmdBuffer-&gt;get(), UP_DIV(output-&gt;elementSize(), 8), 1, 1);
    } else {
        // for NC4HW4 input
        std::vector&lt;VkDescriptorType&gt; types{VK_DESCRIPTOR_TYPE_STORAGE_IMAGE, VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER,
                                            VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER,
                                            VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER};

        switch (mType) {
            case BinaryOpOperation_ADD:
                mBinaryPipeline = mVkBackend-&gt;getPipeline(&#34;glsl_elementwiseAdd_comp&#34;, types);
                break;
            case BinaryOpOperation_MUL:
                mBinaryPipeline = mVkBackend-&gt;getPipeline(&#34;glsl_elementwiseMul_comp&#34;, types);
                break;
            default:
                MNN_PRINT(&#34;Not Supported Binary Operation: %d\n&#34;, mType);
                MNN_ASSERT(false);
                break;
        }

        const int iw = input0-&gt;width();
        const int ih = input0-&gt;height();

        MNN_ASSERT(input0-&gt;dimensions() == input1-&gt;dimensions());

        const int icDiv4 = UP_DIV(input0-&gt;channel(), 4);

        auto binaryOpParam = reinterpret_cast&lt;ConstBuffer*&gt;(mConstBuffer-&gt;map());
        ::memset(binaryOpParam, 0, sizeof(ConstBuffer));

        binaryOpParam-&gt;imgSize[0] = iw;
        binaryOpParam-&gt;imgSize[1] = ih;
        binaryOpParam-&gt;imgSize[2] = icDiv4 * input0-&gt;batch();
        binaryOpParam-&gt;imgSize[3] = 0;

        mConstBuffer-&gt;flush(true, 0, sizeof(ConstBuffer));
        mConstBuffer-&gt;unmap();

        auto sampler = mVkBackend-&gt;getCommonSampler();
        mDescriptorSet.reset(mBinaryPipeline-&gt;createSet());
        mDescriptorSet-&gt;writeImage(reinterpret_cast&lt;VkImageView&gt;(output-&gt;deviceId()), sampler-&gt;get(),
                                   VK_IMAGE_LAYOUT_GENERAL, 0);
        mDescriptorSet-&gt;writeImage(reinterpret_cast&lt;VkImageView&gt;(input0-&gt;deviceId()), sampler-&gt;get(),
                                   VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, 1);
        mDescriptorSet-&gt;writeImage(reinterpret_cast&lt;VkImageView&gt;(input1-&gt;deviceId()), sampler-&gt;get(),
                                   VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL, 2);
        mDescriptorSet-&gt;writeBuffer(mConstBuffer-&gt;buffer(), 3, mConstBuffer-&gt;size());
        mBinaryPipeline-&gt;bind(cmdBuffer-&gt;get(), mDescriptorSet-&gt;get());
        vkCmdDispatch(cmdBuffer-&gt;get(), UP_DIV(iw, 8), UP_DIV(ih, 8), UP_DIV(icDiv4 * input0-&gt;batch(), 4));
    }

    return NO_ERROR;
}
</code></pre></td></tr></table>
</div>
</div><h3 id="opencl-backend">opencl backend</h3>
<p>用EltwiseExecution实现的binaryop</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"><span class="k">class</span> <span class="nc">EltwiseCreator</span> <span class="o">:</span> <span class="k">public</span> <span class="n">OpenCLBackend</span><span class="o">::</span><span class="n">Creator</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
    <span class="k">virtual</span> <span class="n">Execution</span> <span class="o">*</span><span class="n">onCreate</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
                                <span class="k">const</span> <span class="n">MNN</span><span class="o">::</span><span class="n">Op</span> <span class="o">*</span><span class="n">op</span><span class="p">,</span> <span class="n">Backend</span> <span class="o">*</span><span class="n">backend</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">OpType_Eltwise</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">switch</span> <span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">main_as_Eltwise</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">())</span> <span class="p">{</span>
                <span class="k">case</span> <span class="nl">EltwiseType_SUM</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;in0+in1&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">case</span> <span class="nl">EltwiseType_PROD</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;in0*in1&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">case</span> <span class="nl">EltwiseType_MAXIMUM</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;fmax(in0, in1)&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">default</span><span class="o">:</span>
                    <span class="k">break</span><span class="p">;</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="k">nullptr</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">OpType_BinaryOp</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">MNN_ASSERT</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">);</span>

            <span class="k">switch</span> <span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">main_as_BinaryOp</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">opType</span><span class="p">())</span> <span class="p">{</span>
                <span class="k">case</span> <span class="nl">BinaryOpOperation_ADD</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;in0+in1&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">case</span> <span class="nl">BinaryOpOperation_SUB</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;in0-in1&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">case</span> <span class="nl">BinaryOpOperation_MUL</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;in0*in1&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">case</span> <span class="nl">BinaryOpOperation_POW</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;pow(in0, in1)&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">case</span> <span class="nl">BinaryOpOperation_DIV</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;in0/in1&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">case</span> <span class="nl">BinaryOpOperation_MAXIMUM</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;fmax(in0,in1)&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">case</span> <span class="nl">BinaryOpOperation_MINIMUM</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;fmin(in0,in1)&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">case</span> <span class="nl">BinaryOpOperation_REALDIV</span><span class="p">:</span>
                    <span class="k">return</span> <span class="k">new</span> <span class="n">EltwiseExecution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s">&#34;in0/in1&#34;</span><span class="p">,</span> <span class="n">backend</span><span class="p">);</span>
                <span class="k">default</span><span class="o">:</span>
                    <span class="k">break</span><span class="p">;</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="k">nullptr</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="k">nullptr</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">};</span>
</code></pre></td></tr></table>
</div>
</div><p>EltwiseExecution初始化的时候传进来mBroadcast参数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"><span class="n">EltwiseExecution</span><span class="o">::</span><span class="n">EltwiseExecution</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="o">&amp;</span><span class="n">compute</span><span class="p">,</span> <span class="n">Backend</span> <span class="o">*</span><span class="n">backend</span><span class="p">,</span> <span class="kt">float</span> <span class="n">operatorData</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">broadCast</span><span class="p">)</span>
    <span class="o">:</span> <span class="n">CommonExecution</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">mBroadCast</span> <span class="o">=</span> <span class="n">broadCast</span><span class="p">;</span>
    <span class="n">mOperatorData</span> <span class="o">=</span> <span class="n">operatorData</span><span class="p">;</span>
    <span class="n">mBuildOptions</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="s">&#34;-DOPERATOR=&#34;</span> <span class="o">+</span> <span class="n">compute</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>EltwiseExecution::onResize（）函数根据数据存贮类型加载不同的opencl shader完成计算</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-c++" data-lang="c++"> <span class="k">if</span> <span class="p">(</span><span class="n">mBroadCast</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">nhwc_0</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="n">nhwc_1</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span> <span class="p">{</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">nhwc_0</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="p">(</span><span class="n">wh_0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">wh_0</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">?</span>
                            <span class="n">runTime</span><span class="o">-&gt;</span><span class="n">buildKernel</span><span class="p">(</span><span class="s">&#34;binary&#34;</span><span class="p">,</span>
                                <span class="s">&#34;binary_1toM_channel_broadcast_on_awh&#34;</span><span class="p">,</span> <span class="n">mBuildOptions</span><span class="p">)</span> <span class="o">:</span>
                            <span class="n">runTime</span><span class="o">-&gt;</span><span class="n">buildKernel</span><span class="p">(</span><span class="s">&#34;binary&#34;</span><span class="p">,</span>
                                <span class="s">&#34;binary_1toM_channel_broadcast_on_1wh&#34;</span><span class="p">,</span> <span class="n">mBuildOptions</span><span class="p">);</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input0</span><span class="p">));</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input</span><span class="p">));</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">wh_0</span><span class="p">);</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">wh1</span><span class="p">);</span>
                    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="p">(</span><span class="n">wh1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">wh1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">?</span>
                            <span class="n">runTime</span><span class="o">-&gt;</span><span class="n">buildKernel</span><span class="p">(</span><span class="s">&#34;binary&#34;</span><span class="p">,</span>
                                <span class="s">&#34;binary_1toM_channel_broadcast_on_awh&#34;</span><span class="p">,</span> <span class="n">mBuildOptions</span><span class="p">)</span> <span class="o">:</span>
                            <span class="n">runTime</span><span class="o">-&gt;</span><span class="n">buildKernel</span><span class="p">(</span><span class="s">&#34;binary&#34;</span><span class="p">,</span>
                                <span class="s">&#34;binary_1toM_channel_broadcast_on_1wh&#34;</span><span class="p">,</span> <span class="n">mBuildOptions</span><span class="p">);</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input</span><span class="p">));</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input0</span><span class="p">));</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">wh1</span><span class="p">);</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">wh_0</span><span class="p">);</span>
                    <span class="p">}</span>
                    <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]));</span>
                    <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">nhwcArray</span><span class="p">);</span>
                    <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">wh</span><span class="p">);</span>
                <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                    <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">runTime</span><span class="o">-&gt;</span><span class="n">buildKernel</span><span class="p">(</span><span class="s">&#34;binary&#34;</span><span class="p">,</span>
                            <span class="s">&#34;binary_same_channel_broadcast&#34;</span><span class="p">,</span> <span class="n">mBuildOptions</span><span class="p">);</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">wh_0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">||</span> <span class="n">wh_0</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input0</span><span class="p">));</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input</span><span class="p">));</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">wh_0</span><span class="p">);</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">wh1</span><span class="p">);</span>

                    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input</span><span class="p">));</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input0</span><span class="p">));</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">wh1</span><span class="p">);</span>
                        <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">wh_0</span><span class="p">);</span>
                    <span class="p">}</span>
                    <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]));</span>
                    <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">nhwcArray</span><span class="p">);</span>
                    <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">wh</span><span class="p">);</span>
                <span class="p">}</span>
            <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">runTime</span><span class="o">-&gt;</span><span class="n">buildKernel</span><span class="p">(</span><span class="s">&#34;binary&#34;</span><span class="p">,</span> <span class="s">&#34;binary&#34;</span><span class="p">,</span> <span class="n">mBuildOptions</span><span class="p">);</span>
                <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input0</span><span class="p">));</span>
                <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">input</span><span class="p">));</span>
                <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">openCLImage</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]));</span>
                <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">nhwcArray</span><span class="p">);</span>
                <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">wh</span><span class="p">);</span>
                <span class="n">unit</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">input1Stride</span><span class="p">);</span>
            <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>opencl shader</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">__kernel void binary_same_channel_broadcast(__read_only image2d_t input0, __read_only image2d_t input1, __write_only image2d_t output,
                    int4 shape, int2 whInput0, int2 whInput1, int2 whOutput) {
    int2 pos = (int2)(get_global_id(0), get_global_id(1));
    int4 nhwc = (int4)(pos.y/shape.y, pos.y%shape.y, pos.x%shape.z, pos.x/shape.z);
    if (nhwc.x &gt;= shape.x &amp;&amp; nhwc.w &gt;= shape.w)
        return;

    FLOAT4 in0, in1;
    int2 pos0, pos1;

    if (whInput0.x == 1) { // Tensor 0 width length 1
        pos0 = (int2)(nhwc.w*whInput0.x, nhwc.x*whOutput.y+nhwc.y);
        in0 = RI_F(input0, SAMPLER, pos0);
        pos1 = (whInput1.y != 1) ?
            (int2)(nhwc.w*whOutput.x+nhwc.z, nhwc.x*whOutput.y+nhwc.y) :
            (int2)(nhwc.w*whOutput.x+nhwc.z, 0);
    } else if (whInput0.y == 1) { // Tensor 0 height length 1
        pos0 = (int2)(nhwc.w*whOutput.x+nhwc.z, 0);
        in0 = RI_F(input0, SAMPLER, pos0);
        pos1 = (whInput1.x != 1) ?
            (int2)(nhwc.w*whOutput.x+nhwc.z, nhwc.x*whOutput.y+nhwc.y) :
            (int2)(nhwc.w*whInput1.x, nhwc.x*whOutput.y+nhwc.y);
    } else if (whInput0.x == 1 &amp;&amp; whInput0.y == 1) {
        pos0 = (int2)(nhwc.w*whInput0.x, 0);
        in0 = RI_F(input0, SAMPLER, pos0);
        pos1 = (int2)(nhwc.w*whOutput.x+nhwc.z, nhwc.x*whOutput.y+nhwc.y);
    }
    in1 = RI_F(input1, SAMPLER, pos1);
    WI_F(output, pos, OPERATOR);
}
</code></pre></td></tr></table>
</div>
</div>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">carter2005</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2020-01-13
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/training/">training</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2020/2020-01-14_%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">自动微分</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/2020/2020-01-09_python-tips/">
            <span class="next-text nav-default">python技巧</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="carter2005/carter2005.github.io"
            issue-term="title"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:carter2008@gmail.com" class="iconfont icon-email" title="email"></a>
  <a href="https://carter2005.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> 本站总访问量 <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次 </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> 本站总访客数 <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 人 </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2017 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">carter2005</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.d7b7ada643c9c1a983026e177f141f7363b4640d619caf01d8831a6718cd44ea.js"></script>








</body>
</html>
